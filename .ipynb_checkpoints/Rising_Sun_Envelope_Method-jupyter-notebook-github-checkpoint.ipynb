{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Decomposing XANES measurements using the Rising Sun Envelope Method</center> </h1>\n",
    "\n",
    "This is a Supplementary Jupyter-notebook to the paper __*\"The Rising Sun Envelope Method: an automatic and accurate peak location technique for XANES measurements\"*, by R. Monteiro, I. Miyazato, and K. Takahashi.__\n",
    "\n",
    ">__Remark:__ We have decided to not put any function in modules for a didactic purpose; in this way people can understand how each function works and how they have been implemented. \n",
    "\n",
    "The Initial part of the code does the following: \n",
    "1. We open all csv files in the folders (see \"what files\", below). Each file contains $E$ and $\\mu(E)$\n",
    "2. Save all the entries to columns of a pandas vector \"data\", where we alternate $E$ and $\\mu(E)$ (the files do not necessarily have the same length)\n",
    "3. define a materials vector, on which we perform a decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import csv\n",
    "import numpy as np\n",
    "#import seaborn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "### To do normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import multiprocessing \n",
    "import time\n",
    "\n",
    "### To do normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# To prevent weird, long warning\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\",message=\"internal issue\")\n",
    "\n",
    "# where we are saving stuff\n",
    "import os\n",
    "PROJECT_DIR =  os.getcwd()#os.path.dirname(os.path.realpath(__file__))\n",
    "NOTEBOOK = \"XANES_figures\"\n",
    "IMAGES = os.path.join(PROJECT_DIR,\"figures\",NOTEBOOK)\n",
    "\n",
    "## A save figures function\n",
    "def figure_save(figure_name, tight_layout=True, figure_extension=\"eps\",resolution=300):\n",
    "    path = os.path.join(IMAGES,figure_name.replace(\" \", \"_\")+\".\"+figure_extension)\n",
    "    print(\"Saving figure as \",figure_name.replace(\" \", \"_\"))\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path,format=figure_extension,dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Fe data --- List all the csv files in the directory\n",
    "import glob\n",
    "what_files = glob.glob(PROJECT_DIR+\"/Xanes/*.csv\")\n",
    "\n",
    "data=pd.DataFrame([])\n",
    "\n",
    "for name_of_file in what_files:\n",
    "    # read  first row with name of components\n",
    "    aux = pd.read_csv(name_of_file,delimiter=',') \n",
    "    \n",
    "    ## Concatenate them all\n",
    "    data = pd.concat([data,aux],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the organize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(data.columns)[1:]\n",
    "### These are the dictionaries where we will save numpy arrays\n",
    "raw_materials, E, A={}, {}, {}\n",
    "\n",
    "data_val = data.values\n",
    "numb_files = len(what_files)\n",
    "\n",
    "for i in range(1,data.shape[1]):\n",
    "    name = names[i-1]\n",
    "    E[name] = np.reshape(data_val[:,0],(1,-1))\n",
    "    raw_materials[name] = np.reshape(data_val[:,i],(1,-1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements we are dealing with are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ca45acd0cdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfigure_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XANES_some_examples\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = len(names)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(0,L):\n",
    "    plt.plot(E[names[i]].T,raw_materials[names[i]].T,color='C'+str(i%10), label = str(names[i]),lw=4,linestyle='-')\n",
    "\n",
    "plt.title('Xanes spectrum',size=28)\n",
    "plt.ylabel('$\\mu(E)$',size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "plt.legend(loc=4,prop={'size':22})\n",
    "plt.grid(True)\n",
    "figure_save(\"XANES_some_examples\"+names[0])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the first derivative, as provided by Athena: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_files_derivatives = glob.glob(PROJECT_DIR+\"/xafsderivative/Fe_1st_derivative.csv\")\n",
    "\n",
    "data_derivatives=pd.DataFrame([])\n",
    "\n",
    "for name_of_file in what_files_derivatives:\n",
    "    # read  first row with name of components\n",
    "    aux = pd.read_csv(name_of_file,delimiter=',') \n",
    "    \n",
    "    ## Concatenate them all\n",
    "    data_derivatives = pd.concat([data_derivatives,aux],axis=1)\n",
    "\n",
    "\n",
    "names_1st_derivatives = list(data_derivatives.columns)[1:]\n",
    "### These are the dictionaries where we will save numpy arrays\n",
    "raw_materials_1st_derivatives, E_derivatives, A_derivatives={}, {}, {}\n",
    "data_val_derivatives = data_derivatives.values\n",
    "\n",
    "for i in range(1,data_derivatives.shape[1]):\n",
    "    name = names_1st_derivatives[i-1]\n",
    "    E_derivatives[name] = np.reshape(data_val_derivatives[:,0],(1,-1))\n",
    "    raw_materials_1st_derivatives[name] = np.reshape(data_val_derivatives[:,i],(1,-1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_derivatives.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second derivative, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_files_derivatives = glob.glob(PROJECT_DIR+\"/xafsderivative/Fe_2nd_derivative.csv\")\n",
    "\n",
    "data_derivatives=pd.DataFrame([])\n",
    "\n",
    "for name_of_file in what_files_derivatives:\n",
    "    # read  first row with name of components\n",
    "    aux = pd.read_csv(name_of_file,delimiter=',') \n",
    "\n",
    "    ## Concatenate them all\n",
    "    data_derivatives = pd.concat([data_derivatives,aux],axis=1)\n",
    "\n",
    "### These are the dictionaries where we will save numpy arrays\n",
    "raw_materials_2nd_derivatives, E_derivatives, A_derivatives={}, {}, {}\n",
    "data_val_derivatives = data_derivatives.values\n",
    "\n",
    "for i in range(1,data_derivatives.shape[1]):\n",
    "    name = names_1st_derivatives[i-1]\n",
    "    E_derivatives[name] = np.reshape(data_val_derivatives[:,0],(1,-1))\n",
    "    raw_materials_2nd_derivatives[name] = np.reshape(data_val_derivatives[:,i],(1,-1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_materials_2nd_derivatives.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Some auxiliary functions and data normalization </h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things that we should do before we start. First, we shall normalize the data. Note that, as we are only vertically translating or stretching the measurement, the energy component of peaks will not be affected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(materials,names,normalizeheight=False):\n",
    "    '''\n",
    "    This function returns  a vertically translated, and possibly stretched vertically, copy of\n",
    "    the a spectrum to  a vector X, in such a way that \n",
    "    \n",
    "    (i) X[0,0] = 0\n",
    "    holds     and\n",
    "    \n",
    "    (ii) max(X) - min(X)=1  holds when normalizeheight = True\n",
    "    ----------------\n",
    "    Input:\n",
    "    \n",
    "    -names: vector of strings\n",
    "    -materials: dictionary with spectra, indexed by elements in vector names\n",
    "    -normalizeheight: a boolean entry, that defines whether the normalization (**) happens\n",
    "    ----------------\n",
    "    Output is/are:\n",
    "    - X: the renormalized vector\n",
    "    - shift_height: the amout we had to shift the vectoverticaly in order to have X[0.0]=0 \n",
    "    ----------------\n",
    "    '''\n",
    "    shift_height, normalization_heights, X={}, {}, {}\n",
    "    for name in names:\n",
    "        shift_height[name] = np.copy(materials[name][0,0])\n",
    "        X[name]  = materials[name]- shift_height[name]\n",
    "    \n",
    "    if normalizeheight:\n",
    "        ### Now we use the function normalize, from sklearn\n",
    "        for name in names:\n",
    "            X[name],normalization_heights[name] = normalize(X[name], norm='max', axis=1,return_norm=True)\n",
    "\n",
    "    return X, shift_height, normalization_heights\n",
    "\n",
    "#########################################################################################################\n",
    "def oscillation_function(interval,f,full_computation=True):\n",
    "    ''' set an oscillation function f on an interval interval.\n",
    "    \n",
    "    ----------------\n",
    "    Input:\n",
    "    \n",
    "    - interval of computation: THIS IS NOT NECESSARY, ACTUALLY!\n",
    "    - f: the function whose oscillation will be computed\n",
    "    - full_computation, a flag, where user define whether to compute fulll oscillation function, \n",
    "      or just the threshold\n",
    "    \n",
    "    ----------------\n",
    "    Output:\n",
    "    \n",
    "    - x: a vector labeled as 1...N\n",
    "    - oscillation: oscillation of the function f\n",
    "    - estimated oscillation: the threshold oscillation, given as oscillation[0,1]\n",
    "    ----------------\n",
    "    '''\n",
    "    #plot the oscillation of the function\n",
    "    L = interval.shape[1]\n",
    "    x = np.reshape(np.arange(L),(1,-1))\n",
    "    oscillation = np.zeros([1,L]).reshape(1,-1)\n",
    "    \n",
    "    if full_computation:\n",
    "        for l in range(1,L):\n",
    "            oscillation[0,l] = np.max([np.max(f[0,k:k+l+1]) - np.min(f[0,k:k+l+1]) for k in range(L-l)])\n",
    "        \n",
    "        return (x, oscillation, oscillation[0,1])\n",
    "    else:\n",
    "        oscil_number = np.max([np.max(f[0,k:k+1+1]) - np.min(f[0,k:k+1+1]) for k in range(L-1)])\n",
    "        \n",
    "        return (x, None, oscil_number)\n",
    "#########################################################################################################    \n",
    "def plateau_detection (v,L_threshold):\n",
    "    '''\n",
    "    This function detects plateaus of length > L_threshold in a vector v. \n",
    "    ----------------\n",
    "    Input:\n",
    "    - v: a matrix with shape 1 X m\n",
    "    - L_threshold: plateaus with length L < L_threshold will not be considered]. L_threshold >=2\n",
    "    ----------------\n",
    "    Output:\n",
    "    \n",
    "    It returns their \n",
    "    - location: the entry of the peak in the vector\n",
    "    - the value that the vector v take at that point.\n",
    "    \n",
    "    If no peak is found then it returns (None, None)\n",
    "    ----------------\n",
    "    '''\n",
    "    v_temp = np.copy(v).reshape(1,-1) # this is a row vector, not a rank one vector\n",
    "    \n",
    "    # In order to detect the plateaus, we make a copy of the vector and compare it with a shifted version of itself\n",
    "    v_aux =np.copy(v_temp[0,0:-1]) ## a smaller version of v\n",
    "    v_shifted = np.copy(v_temp[0,1:])\n",
    "    ''' They are going to be use in the following fashion:\n",
    "        anytime we have a number 1 means that the next element is the same to the next one.\n",
    "        The length of a sequence of 1s defines the length of that sequence'''\n",
    "        \n",
    "    # Now we compare v_aux and v_shifted, not forgetting to add 0's in the beggining and in the end;\n",
    "    # We do that because we also want to find sequences in the extremes of the vector\n",
    "    where_plts_start_and_length = np.array(1*(v_aux==v_shifted),ndmin=2)\n",
    "    where_plts_start_and_length = np.concatenate([[0],where_plts_start_and_length[0],[0]]).reshape(1,-1)\n",
    "            \n",
    "    '''\n",
    "    NOTE #1: where_plts_start_and_length is avector with schape 1X m+1\n",
    "    NOTE #2: sequences of 1`s with length a in where_plts... indicate sequnces in v with length a+1\n",
    "    \n",
    "     At this point we define sequences of 010, 0110 etc, and look for them in the vector.\n",
    "     If the number of 1's is strictly less than L_threshold -1 then we ignore the plateau.\n",
    "     '''\n",
    "    for i in range(1,L_threshold-1):  \n",
    "        # we shall ignore strings that are smaller than L_threshold\n",
    "        # now we define a list of 1s, with legth i\n",
    "        sequence = np.ones([1,i],dtype=np.int32)\n",
    "        # 1- AUGMENTING THE VECTOR WITH ZEROS\n",
    "        sequence = np.concatenate([[0],sequence[0],[0]]).reshape(1,-1)\n",
    "        \n",
    "        # Now we search for it in the vector where_plts_start_and_length\n",
    "        range_search = where_plts_start_and_length.shape[1]-sequence.shape[1]+1\n",
    "        \n",
    "        is_there = [j for j in range(range_search) if str(where_plts_start_and_length[0,j:j+sequence.shape[1]]) ==str(sequence.squeeze()) ]      \n",
    "        if is_there != []:\n",
    "            for k in is_there :\n",
    "                where_plts_start_and_length[0,k:k+sequence.shape[1]] = np.zeros([1,sequence.shape[1]])## erase that small string\n",
    "        \n",
    "    '''\n",
    "    2- at this point the vector where_plts_start_and_length has no sequence of 1's with length < L_threshold -1\n",
    "    the first maximum will be at \n",
    "    '''\n",
    "     \n",
    "    if (np.sum(where_plts_start_and_length) ==0):\n",
    "        #print(\"No peaks!\")\n",
    "        return (None,None)\n",
    "    \n",
    "    Len_where = where_plts_start_and_length.shape[1]-1\n",
    "    locations = np.array([],np.int16,ndmin=2)\n",
    "    \n",
    "    for i in range(Len_where):\n",
    "        # Whenever we have a seq of 1s, we know for sure that it has length bigger or equal to L_threshold,\n",
    "        # so we can return the value of it`s location\n",
    "        \n",
    "        if (where_plts_start_and_length[0,i+1]- where_plts_start_and_length[0,i] ==1):\n",
    "            locations = np.concatenate((locations,np.array([i],np.int16, ndmin=2)),axis=1) # subtract 1, because the vector was augmented by 0 in the extremes\n",
    "        \n",
    "    locations = np.reshape(locations[0], (1,-1))\n",
    "    return (locations[0],v[0,locations[0]])\n",
    "    \n",
    "\n",
    "#########################################################################################################\n",
    "def start_from_peak(Maj,L_threshold):\n",
    "    '''\n",
    "    Given a majorating function that starts with a plateau, find its second plateau, as long as the \n",
    "    latter has length bigger than L_threshold.\n",
    "    ----------------\n",
    "    Input:\n",
    "    -Maj: a rising sun function\n",
    "    -L_threshold: the length of the plateaus that should be taken into account \n",
    "    ----------------\n",
    "    Output:\n",
    "    -location: index of the next plateau\n",
    "    - value: value of the function at the next plateau\n",
    "    ----------------\n",
    "    '''\n",
    "    \n",
    "    L = Maj.shape[1];\n",
    "    m = Maj[0,0];\n",
    "    i=1;\n",
    "    while(i<L-2 and m == Maj[0,i]):\n",
    "            i= i+1;\n",
    "    \n",
    "    locations,value = plateau_detection(np.array(Maj[0,i:],ndmin=2),L_threshold);\n",
    "    return (locations+i, value)\n",
    "    \n",
    "\n",
    "#########################################################################################################\n",
    "def padded_spectra(E, materials, names):\n",
    "    '''\n",
    "    Embeds Energy, XANES measurement spectrum into larger space, padding the Xanes measurement by a constant.\n",
    "    Returns  emebedded Energy, embedded Xanes measurement, and index\n",
    "    ----------------\n",
    "    Input:\n",
    "    - E: dictionary indexed by names, with energy interval\n",
    "    -mu: dictionary indexed by names, with XANES measurements\n",
    "    -names: keys of the previous dictionaries with names of materials\n",
    "    \n",
    "    ----------------\n",
    "    Output:\n",
    "    \n",
    "    - padded_energy: dictionary indexed by names containing an extended energy vector to the right\n",
    "    - padded_material: dictionary indexed by names containing a padded (o the right, by constant) xanes measurement\n",
    "    -stopping: index of the last entry before the padding starts\n",
    "    ----------------\n",
    "    '''\n",
    "    \n",
    "    padded_material,  padded_energy, stopping ={}, {}, {}\n",
    "    \n",
    "    for name in names:\n",
    "        stopping[name]= materials[name].shape[1]\n",
    "        # the material vector needs to be padded....\n",
    "        material_aux = np.squeeze(materials[name])\n",
    "        #... while the energy vector needs to be extended \n",
    "        energy_aux = E[name] +E[name][0,-1]+E[name][0,1]-2*E[name][0,0]\n",
    "        \n",
    "        padded_material[name] = np.reshape(np.pad(material_aux,(0,stopping[name]),'edge'),(1,-1))\n",
    "        energy_aux = np.concatenate((E[name],energy_aux),axis=1)\n",
    "        padded_energy[name] = np.reshape(energy_aux,(1,-1))\n",
    "        \n",
    "    return padded_energy,padded_material,stopping\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "def small_padded_spectra(E, F):\n",
    "    '''\n",
    "    Embeds two vectors E and F into larger space, extending E in one length to the right and\n",
    "    padding the F measurement by a constant.\n",
    "    Returns  emebedded Energy, embedded Xanes measurement.\n",
    "    \n",
    "    Unlike the function padded_spectra(E, materials, names), it does not need names\n",
    "    ----------------\n",
    "    Input:\n",
    "    - E: numpy array denoting  energy interval\n",
    "    - F: numpy array  denoting  XANES measurements\n",
    "    ----------------\n",
    "    Output:\n",
    "    \n",
    "    - padded E: numpy array  containing an extended energy vector to the right\n",
    "    - padded_ F: numpy array  containing a padded (o the right, by constant) xanes measurement\n",
    "    -stopping: index of the last entry before the padding starts\n",
    "    ----------------\n",
    "    '''\n",
    "    stop= E.shape[1]\n",
    "    # the material vector needs to be padded....\n",
    "    F_aux = np.squeeze(F)\n",
    "        #... while the energy vector needs to be extended \n",
    "    energy_aux = E +E[0,-1]+E[0,1]-2*E[0,0]\n",
    "        \n",
    "    padded_F = np.reshape(np.pad(F_aux,(0,stop),'edge'),(1,-1))\n",
    "    energy_aux = np.concatenate((E,energy_aux),axis=1)\n",
    "    padded_energy = np.reshape(energy_aux,(1,-1))\n",
    "        \n",
    "    return padded_energy,padded_F,stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is Rising Sun function is based on the Rising Sun Lemma, of Riesz: https://en.wikipedia.org/wiki/Rising_sun_lemma. The idea is that this function is more well behaved, and is less oscillatory. See also https://terrytao.wordpress.com/2010/10/16/245a-notes-5-differentiation-theorems/#more-4290 for a nice overview of the technique's use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rising_sun(mu):\n",
    "    '''\n",
    "    Given L = length of mu, return a vector maxim_mu of length L,for which, at any index \n",
    "     0 <=k <L we have  maxim_mu[k] >= maxim_mu[j],for all 0 <=j <k  \n",
    "     \n",
    "    ----------------\n",
    "    Input:\n",
    "    -mu: the XANES measurement\n",
    "    \n",
    "    ----------------\n",
    "    Output:\n",
    "    - maxim_mu: therising_sun function  associated to mu\n",
    "    ----------------\n",
    "     '''\n",
    "    L = mu.shape[1]\n",
    "    maxim_mu = np.zeros([1,L])\n",
    "    maxim_mu[0,0]=mu[0,0]\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        if mu[0,i]>maxim_mu[0,(i-1)]:\n",
    "            maxim_mu[0,i] = mu[0,i]\n",
    "        else:\n",
    "            maxim_mu[0,i] = maxim_mu[0,i-1]\n",
    "    \n",
    "    return maxim_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the previous functions to normalize the spectrum and embedd in in a bigger space of continuous functions; the main reason to do the latter is that it will help us to know when to stop the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials, shift_height, normalizer_shift =normalization(raw_materials,names,normalizeheight=True)\n",
    "\n",
    "E_padded,materials, stopping = padded_spectra(E,materials,names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the main difference is that the padded version is twice as big as the non-padded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(E[names[1]]), np.shape(E_padded[names[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Setting up parameters and hyperparameters  </h2></center>\n",
    "\n",
    "The next thing we shall do is to create a parameter dictionary, that will contain a lot of information about each xanes measurement. In fact it will be a dictionary of dictionaries\n",
    "\n",
    "\n",
    "Now let's start \"feeding\" the parameter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={}\n",
    "\n",
    "for name in names:\n",
    "    parameters[name]={\n",
    "        # The non-normalized xanes\n",
    "        'raw_xanes': raw_materials[name],\n",
    "        'raw_energy': E[name],\n",
    "   \n",
    "        # The normalized xanes and embedded xanes\n",
    "        'xanes': materials[name],\n",
    "        'energy': E_padded[name],\n",
    "\n",
    "        # The normalization and emebdding info\n",
    "        'stop': stopping[name],\n",
    "        'shift_height': shift_height[name],\n",
    "        'normalizer_shift': normalizer_shift[name]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we are going to need are hyperparameters: I will keep them all the same throught the analysis for all the materials, but it doesn't have to be always the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_h':1,\n",
    "    'lambda_d':1/2,\n",
    "    \n",
    "    ## These hyperparameters will be used in find_first_peak\n",
    "    'lambda_h_find_1st':4,\n",
    "    'lambda_d_find_1st':1/4,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_d_shrink_1st':1/2, \n",
    "    'initial_oscillation_guess_parameter':10,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'stretching_factor':3,\n",
    "    'iteration_decay':.9,\n",
    "    \n",
    "    ## Type of decay_rate\n",
    "    'decay_rate_type': 'min_max'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side by side, the normalized and non-normalized look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's replot, normalizing first\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "i=0\n",
    "\n",
    "for name in {names[1]}:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now= parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "    plt.plot(E_now_unpad.T,xanes_now_unpad.T,color='C0', label = str(names[i])+'non-normalized',lw=4,linestyle='-')\n",
    "    plt.plot(E_now.T,xanes_now.T,color='C1', label = str(names[i])+'normalized and padded',lw=4,linestyle='-')\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "plt.title('Embedded spectrum using padding', size=28)\n",
    "plt.ylabel('$\\mu(E)$',size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "plt.legend(loc=4,prop={'size':22})\n",
    "plt.grid(True)\n",
    "figure_save(\"XANES_some_examples_normalized_and_embedded\"+names[0])\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Applying the Rising Sun operator </h2></center>\n",
    "\n",
    "Now we plot the spectrum with corresponding Rising Sun function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(names)\n",
    "\n",
    "rising_sun_f ={}\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "i=0  # a flag, used to vary the plot colors\n",
    "\n",
    "for name in names:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "\n",
    "    rising_sun_f[name] = rising_sun(xanes_now)\n",
    "    plt.plot(\n",
    "        E_now.T,rising_sun_f[name].T,color='C'+str(i%10),\\\n",
    "        label = '$\\mathscr{R}_{\\mu}(\\cdot)$ of '+str(name),marker='o',markersize=4,lw=3,linestyle=':')\n",
    "    plt.plot(E_now.T,xanes_now.T,color='C'+str(i%10), label ='$\\mu(\\cdot)$ of '+ str(name),lw=3,linestyle='--')\n",
    "    i+=1\n",
    "    \n",
    "plt.title('Rising sun function',size=28)\n",
    "plt.ylabel('$\\mathscr{R}_{\\mu}(E)$',size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "plt.legend(loc=4,prop={'size': 22})\n",
    "plt.grid(True)\n",
    "figure_save(\"XANES_and_rising_sun\"+names[0])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the Valley of Shadows function, initially defined as\n",
    "\n",
    "$$ \\mathcal{V}_{\\mu}(\\cdot) =\\mathcal{R}_{\\mu}(\\cdot) -  \\mu(\\cdot)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_of_shadows = {}\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "i=0\n",
    "\n",
    "for name in names:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "\n",
    "    valley_of_shadows[name] = rising_sun_f[name]- xanes_now #this function will always be nonnegative, thanks to (1)\n",
    "    plt.plot(E_now.T,valley_of_shadows[name].T,color='C'+str(i%10), label =name,lw=4,linestyle='-')\n",
    "    i+=1\n",
    "\n",
    "plt.title('Valley of Shadows function',size=28)\n",
    "plt.legend(loc=4,prop={'size': 22})\n",
    "plt.ylabel(r\"$\\mathcal{V}_S[f]$\",size=28)\n",
    "plt.xlabel('Energy (eV)',size=28)\n",
    "plt.grid(True)\n",
    "figure_save(\"valley_of_shadows\"+names[0])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give an idea of how these functions encode information, we plot one of them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for name in {names[1]}:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "\n",
    "    plt.plot(E_now[0,0:stopping[name]].T,xanes_now[0,0:stopping[name]].T,\\\n",
    "             color='C'+str(i%10),lw=4,linestyle='--', label = r\"$\\mu(\\cdot)$: \"+ str(name))\n",
    "    plt.plot(E_now[0,0:stopping[name]].T,rising_sun_f[name][0,0:stopping[name]].T,\\\n",
    "             color='red',lw=4,linestyle='-', label = r\"$\\mathcal{R}_{\\mu}(\\cdot)$ :\"+ str(name),markersize=2)\n",
    "    plt.plot(E_now[0,0:stopping[name]].T,valley_of_shadows[name][0,0:stopping[name]].T,\\\n",
    "             color='blue',linestyle='-.',lw=4, label = r\"$\\mathcal{V}_{\\mu}(\\cdot)$ : \"+ str(name))\n",
    "\n",
    "plt.title(\n",
    "    r\"The (non-embedded) functions $\\mu(\\cdot),  \\mathcal{V}_{\\mu}(\\cdot)$ and $\\mathcal{R}_{\\mu}(\\cdot)$\",\n",
    "    size=28)\n",
    "plt.ylabel(name+\" and\\n auxiliar functions\",size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "\n",
    "plt.legend(loc=4,prop={'size': 22})\n",
    "plt.grid(True)\n",
    "figure_save(\"XANES_and_rising_sun\"+names[1])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Estimating the oscilation and regularity</h2></center>\n",
    "\n",
    "We designed a non-parametric way to estimate the oscillation\n",
    "\n",
    ">Note: even though we are compiting the oscillation for the embedded version, this is not necessary. The invariants that we constructed also leave the oscillation, and peak locations invariant.<\n",
    "\n",
    "\n",
    "The definition of the Oscillation function, as given by Def. I.4 in the paper, where we say that \n",
    "\n",
    " \\begin{align}\n",
    "\\omega_*^{\\mu}(j \\delta) = \\max_{\\vert E - E'\\vert\\leq j \\delta } \\vert \\mu(E) - \\mu(E')\\vert, \\tag{oscillation function}\n",
    " \\end{align}\n",
    "  is called  \\textit{oscillation function} of $\\mu(E)$; we set\n",
    " \n",
    " $$h_*= \\omega_*^{\\mu}(\\delta)$$\n",
    " \n",
    " (or this quantity multiplied by a hyperparameter factor) as our trhreshold for height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oscillation, interval, estimate={}, {}, {}\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for name in names:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "\n",
    "    (interval[name],oscillation[name], estimatE_now) = oscillation_function(E_now,xanes_now)\n",
    "    new_label = name+\", noise:\" +str(round(estimatE_now,4))\n",
    "    plt.plot(interval[name].T,oscillation[name].T,linestyle='-',lw=4, label=new_label)\n",
    "\n",
    "plt.title('Oscillation function $\\omega$',size=28)\n",
    "plt.legend(loc=4,prop={'size': 22})\n",
    "plt.ylabel('$\\omega$',size=28)\n",
    "plt.xlabel('Spreading size',size=28)\n",
    "plt.grid(True)\n",
    "figure_save(\"Oscillation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll compute just the oscillation function of the Fe_calc to put in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for name in {names[1]}:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "\n",
    "    (interval[name],oscillation[name], estimatE_now) = oscillation_function(E_now,xanes_now)\n",
    "    new_label = name+\", noise:\" +str(round(estimatE_now,4))\n",
    "    plt.plot(interval[name].T,oscillation[name].T,linestyle='-',lw=4, label=new_label)\n",
    "\n",
    "plt.title('Oscillation function $\\omega$',size=28)\n",
    "plt.legend(loc=4,prop={'size': 22})\n",
    "plt.ylabel('$\\omega$',size=28)\n",
    "plt.xlabel('Spreading size',size=28)\n",
    "plt.grid(True)\n",
    "figure_save(\"Oscillation\"+names[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to illustrate one of the theorems in the paper, we plot the oscillation function of the Rising sunf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for name in {names[0]}:\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    xanes_now_unpad = parameters_now['raw_xanes']\n",
    "    E_now_unpad =parameters_now['raw_energy']\n",
    "    \n",
    "    f = plt.figure(figsize=(15,7))\n",
    "    \n",
    "    ax1 = plt.subplot(211)\n",
    "    new_label = name+\", noise:\" +str(round(estimatE_now,4))\n",
    "    ax1.plot(interval[name].T,oscillation[name].T,linestyle='-',lw=2, label=\"$\\omega^{\\mu}$: \"+name,color='r')\n",
    "    \n",
    "    (interval_rising,oscillation_rising, _) = oscillation_function(E_now,rising_sun_f[name])\n",
    "    ax1.plot(interval_rising.T,oscillation_rising.T,linestyle='-',\\\n",
    "             lw=2, label=\"$\\omega^{\\mathscr{R}_{\\mu}}$: \"+name,color='gray')\n",
    "    \n",
    "    ax1.title.set_text('Oscillation function of $\\omega^{\\mu}$ and $\\omega^{\\mathscr{R}_{\\mu}}$')\n",
    "    ax1.title.set_fontsize(28)\n",
    "    ax1.legend(loc=4,prop={'size': 22})\n",
    "    ax1.set_ylabel('$\\omega$',size=28)\n",
    "    ax1.grid(True)\n",
    "     \n",
    "    ax2 = plt.subplot(212)\n",
    "    new_label = name+\", noise:\" +str(round(estimatE_now,4))\n",
    "    ax2.plot(interval[name][0,50:200].T,oscillation[name][0,50:200].T,linestyle='-',lw=2,\\\n",
    "             label=\"$\\omega^{\\mu}$: \"+name,color='r')\n",
    "    (interval_rising,oscillation_rising, _) = oscillation_function(E_now,rising_sun_f[name])\n",
    "    ax2.plot(interval_rising[0,50:200].T,oscillation_rising[0,50:200].T,linestyle='-',\\\n",
    "             lw=2, label=\"$\\omega^{\\mathscr{R}_{\\mu}}$: \"+name,color='gray')\n",
    "    ax2.legend(loc=4,prop={'size': 22})\n",
    "    ax2.grid(True)\n",
    "    \n",
    "figure_save(\"Oscillation denoising\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, one could compute the number of elements in the oscillation of Rising sun function that are above the oscillation of the xanes funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(1*(oscillation_rising>oscillation[names[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(1*(oscillation_rising==oscillation[names[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(1*(oscillation_rising<oscillation[names[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is, these functions differ only in a small set (compared to the whole size of the domain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Auxiliary function to compute decay rate</h2></center>\n",
    "\n",
    "The following functions compute the decay rate in two possible ways:\n",
    "1. computing averages\n",
    "2. computing exponential decay using least squares\n",
    "\n",
    "The advantage of using exponential decay is that it guarantees that the estimated height will be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "   \n",
    "def decay_rate(\n",
    "               parameters,hyperparameters,name,omega_oscillation,\\\n",
    "               jumps,vector_height_threshold,vector_peak_loc,\\\n",
    "               distances,j\n",
    "):\n",
    "    '''\n",
    "    Given a vector with length N, compute the decay either as average or as a exponential rate of decay\n",
    "    decay_rate_type == min_max or 'regression'\n",
    "    ----------------\n",
    "    Input:\n",
    "    -parameters: dictionary with materials' properties\n",
    "    -hyperparameters: hyperparamters dictionary\n",
    "    -name: name of material whose decay is being studied\n",
    "    -omega_oscillation: oscillation function\n",
    "    -jumps: jumps[i] gives the value |intensity_peak[i-1] - intensity_peak[i]|\n",
    "    -vector_height_threshold: the vector with the series (h_*^{(0)}, h_*^{(1)}, ..., h_*^{(j)})\n",
    "    -vector_peak_loc: vector with peak locations (oth peak, 1st peak,...., jth peak)\n",
    "    -distances: distance between peaks, up to distance between peak[j-1] and peak[j]\n",
    "    -j: last peak that we have found\n",
    "    ----\n",
    "    \n",
    "    It depends on the decay_rate_type:\n",
    "    -If decay_rate_type == min_max:\n",
    "        - hyperparameters used: lambda_d and lambda_h\n",
    "        ## See paper for further explanation\n",
    "    \n",
    "    -If decay_rate_type == reversed:\n",
    "        Uses the min-max method in the reverse passage.\n",
    "        - hyperparameters used:lambda_d and lambda_h\n",
    "        ## See paper for further explanation\n",
    "    \n",
    "    -If decay_rate_type == learn_to_trust:\n",
    "        Uses same method as min_max to estimate h_threshold, but uses weighted method to average distance\n",
    "        - hyperparameters used: lambda_d and lambda_h\n",
    "    \n",
    "    -If decay_rate_type == regression:\n",
    "        Uses regression in a weighted fashio to estimate distance/plateaus size (see paper, equation (8))\n",
    "        \n",
    "        For the height threshold, uses a regression to estimate an exponential fit to XANES curve \n",
    "        to the right of jth-peak, then uses covariance matrix to estimate amount of oscillation in the remaining \n",
    "        curve in the same interval\n",
    "    \n",
    "        - hyperparameters used: lambda_d,  lambda_h, and stretching_factor\n",
    "    ----------------\n",
    "    Output:\n",
    "    ----------------\n",
    "    -decay_factor: alpha_n in the paper\n",
    "    - dist_estimated: estimated distance/plateaus size\n",
    "    - height_threshold: estimated height threshold\n",
    "    - decay_plot dictionary with information about regression (only used when decay_rate_type == regression)\n",
    "    \n",
    "    ----------------\n",
    "    '''\n",
    "    # Unpacking ....\n",
    "    # ...parameters\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    stop = parameters_now['stop']    \n",
    "\n",
    "    # ... hyperparameters\n",
    "    lambda_d = hyperparameters['lambda_d']\n",
    "    lambda_h = hyperparameters['lambda_h']\n",
    "    decay_rate_type = hyperparameters['decay_rate_type']\n",
    "    start = vector_peak_loc[j-1]+1 #peak_loc[str(j-1)+name]\n",
    "    \n",
    "    ###################################\n",
    "    if (decay_rate_type =='min_max'):\n",
    "        decay_factor = (np.max(xanes_now[0,start:]) -np.min(xanes_now[0,start:]))/omega_oscillation[0,-1]\n",
    "        height_threshold = decay_factor*lambda_h*vector_height_threshold[j-1]\n",
    "        \n",
    "        #### Old way\n",
    "        ##dist_estimated = int( max(decay_factor,lambda_d)*np.average(distances))\n",
    "        dist_estimated = max(int( max(decay_factor,lambda_d)*np.average(distances)),2)\n",
    "            \n",
    "        return decay_factor, dist_estimated,height_threshold,None\n",
    "    \n",
    "    ###################################\n",
    "    elif decay_rate_type =='reversed':\n",
    "        \n",
    "        decay_factor = (np.max(xanes_now[0,start:]) -np.min(xanes_now[0,start:]))/omega_oscillation[0,-1]\n",
    "        height_threshold = decay_factor*lambda_h*vector_height_threshold[j-1]\n",
    "        dist_estimated = max(int(lambda_d*np.average(distances)),2)\n",
    "            \n",
    "        return decay_factor, dist_estimated,height_threshold,None\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    elif decay_rate_type == 'learn_to_trust':\n",
    "        \n",
    "        decay_factor = (np.max(xanes_now[0,start:]) -np.min(xanes_now[0,start:]))/omega_oscillation[0,-1]\n",
    "        height_threshold = decay_factor*lambda_h*vector_height_threshold[j-1]\n",
    "        ## Take into account that j is always greater than 1\n",
    "        dist_estimated = max( int(((lambda_d*np.exp(-(j-1)) +(j-1)/2)/j)*2*np.average(distances)),2)\n",
    "            \n",
    "        return decay_factor, dist_estimated,height_threshold,None\n",
    " \n",
    "    ###################################\n",
    "  \n",
    "    else: #decay_rate_type = 'regression':\n",
    "        decay_plot={}\n",
    "        ###################################\n",
    "        # We begin with aa log-regression for distance\n",
    "        lin_reg_dist = LinearRegression()\n",
    "        interval = np.reshape(np.arange(j),(-1,1))\n",
    "        lin_reg_dist.fit(interval,np.log(np.reshape(distances,(-1,1))) )\n",
    "        predicted_distance = np.exp(lin_reg_dist.predict(np.reshape(interval[-1,0],(-1,1))))\n",
    "        dist_estimated = max( int(((lambda_d*np.exp(-(j-1)) +(j-1)/2)/j)*2*predicted_distance),2)\n",
    "        \n",
    "        decay_plot['regression_for_distance'] =lin_reg_dist\n",
    "        \n",
    "        ###################################\n",
    "        \n",
    "        #Then we use the estimated distance to do a regression for the height threshold\n",
    "        stretching_factor= hyperparameters['stretching_factor']\n",
    "        lin_reg_jumps = LinearRegression()\n",
    "        E_for_pred = np.reshape(E_now[0,start:start+int(2*dist_estimated)],(-1,1))\n",
    "        xanes_for_pred = np.reshape(xanes_now[0,start:start+int(2*dist_estimated)],(-1,1)) \n",
    "\n",
    "        lin_reg_jumps.fit(E_for_pred,xanes_for_pred)\n",
    "\n",
    "        # predicted values. \n",
    "        pred = lin_reg_jumps.predict(E_for_pred)\n",
    "        v = xanes_for_pred - pred\n",
    "\n",
    "        residual = np.power(np.linalg.norm(v),2)/(len(E_for_pred)-2)\n",
    "        X = np.c_[np.ones([len(E_for_pred),1]),E_for_pred]\n",
    "        \n",
    "        ## Preparation steps to use the covariance matrix\n",
    "        M = np.linalg.inv(np.matmul(np.transpose(X),X))\n",
    "\n",
    "        error = np.sqrt(np.diag(np.matmul(np.matmul(X,M),X.T))*residual)\n",
    "        error = np.reshape(error,(-1,1))\n",
    "        decay_plot ={'range_energy_pred':E_for_pred,'range_xanes_pred':pred,'error':error}        \n",
    "\n",
    "        jumps_average = np.average(jumps)\n",
    "        \n",
    "        min_est = min(\n",
    "            xanes_for_pred[0,0]- stretching_factor*error[0,0],\\\n",
    "            xanes_for_pred[-1,0]- stretching_factor*error[-1,0] \n",
    "        )\n",
    "        max_est= max(\n",
    "            xanes_for_pred[0,0]+ stretching_factor*error[0,0],\\\n",
    "            xanes_for_pred[-1,0]+ stretching_factor*error[-1,0] \n",
    "        )\n",
    "        estimated_jump = max_est- min_est\n",
    "        ##########################################\n",
    "        decay_factor = estimated_jump/jumps_average\n",
    "        height_threshold = decay_factor*vector_height_threshold[0]#np.average(vector_height_threshold)\n",
    "        \n",
    "        return decay_factor, dist_estimated,height_threshold,decay_plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Decomposing the XANES spectrum  </h2></center>\n",
    "\n",
    "\n",
    "One of the biggest concerns we have in the beginning is having a good estimate for the distance between peaks. A good estimate would be\n",
    "\n",
    "$$ \\Delta \\approx \\vert x_1 - x_0 \\vert, \\quad \\text{where} \\quad \\min\\mu = \\mu(x_0) \\quad \\text{and} \\quad \\max\\{\\mu\\} - \\mu(x_1).$$\n",
    "\n",
    "The issue with this idea is due to (i) noise, which makes the minimum to get possibly concentrated on the extreme left; (ii) the maximum to be on the extreme right. \n",
    "\n",
    "An idea to avoid these issues is the following: to overcome the first issue, we define $x_1$ as\n",
    "\n",
    "\\begin{align}\n",
    "x_1 &:= e_1\n",
    "\\end{align}\n",
    "\n",
    "where e_1 is a local maximum with respect to $e_{-\\infty}$ with threshold $\\left(\\min(5h_*,.5), \\frac{e_{\\infty}-e_{-\\infty}}{4}\\right)$ and posteriorly, we define $x_0$ as\n",
    "\n",
    "\\begin{align}\n",
    "x_0 &:= \\sup_{E\\in (e_0,x_1)}\\left\\{\\,E\\,\\Big|\\,\\mu(E) <\\mu(e_{-\\infty})+\\omega(\\mu)\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "The benefits are clear: we overcome the issue of concentration on the extremities, finding a nice estimate for the size of the distance between heights and peaks. This algorithm is implemented below.\n",
    "\n",
    "\n",
    "> This part of the code is very delicate and critical for a good algorithm bootstrap. I migth try some variations on it based on the size of the first plateau; it will be costier, but will allow us to avoid the issue of dependence on the maximum that stays close to the right end-point of the spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_distance(parameters,hyperparameters,name,rising_sun,oscillation):\n",
    "    '''\n",
    "    This function estimate the distance between local maxixanes_nowm and minixanes_nown in order to bootstrap the algorithm. \n",
    "    The first step consists in estimating x_1, which will then be e_1\n",
    "    and posteriorly estimates x_0\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - hyperparameters: hyperparamters dictionary\n",
    "    - name: name of material whose decay is being studied\n",
    "    - rising_sun: the rising sun function associated with xanes_now\n",
    "    - oscillation, which given the intrinsic oscillation of xanes_now\n",
    "    ----------------\n",
    "    Output:\n",
    "    The function output are\n",
    "    (x_0,x_1): the location of the minixanes_nowm and that of the maxixanes_nowm in the half-interval  \n",
    "    ----------------\n",
    "    '''\n",
    "    # Unpacking\n",
    "    xanes_now= parameters[name]['xanes']\n",
    "    lambda_h_find_1st = hyperparameters['lambda_h_find_1st']\n",
    "    lambda_d_find_1st = hyperparameters['lambda_d_find_1st']\n",
    "    \n",
    "    ## We want tthe raw xanes measurement, prior to the embedding, so we do the following\n",
    "    m = xanes_now.shape[1];\n",
    "    new_xanes_now = np.array(xanes_now[0,0:int(m/2)],ndmin=2)  \n",
    "        \n",
    "    '''\n",
    "    In the next part we choose the smallest argument for which the spectrum assumes its maxixanes_nowm point\n",
    "    #x_1 = min(np.where(np.max(new_xanes_now)==new_xanes_now)[1])\n",
    "    A second version consists of finding the first peak with thresholds (4*oscillation, length/4)\n",
    "    '''\n",
    "    ################################################################  \n",
    "    locations, heights = plateau_detection(rising_sun,int(lambda_d_find_1st*m+1))\n",
    "    \n",
    "    # Now we retain only the elements that are higher than the threshold\n",
    "    what_matters= np.where(heights>=min(lambda_h_find_1st*oscillation,.5))    ### Lmabda_h ==5 before\n",
    "    locations, heights = locations[what_matters], heights[what_matters]\n",
    "    x_1 = locations[0]\n",
    "    #################################################################\n",
    "    \n",
    "    min_xanes_now = max(np.min(new_xanes_now), rising_sun[0,0]) ## WE do this in case the function starts in a valley\n",
    "    \n",
    "    x_0 = np.copy(x_1)\n",
    "    while( new_xanes_now[0,x_0]>=min_xanes_now + oscillation): \n",
    "        x_0 =x_0 - 1\n",
    "    # obviously x_0 =< x_1\n",
    "    \n",
    "    return (x_0, x_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a major change in the next code: now the code adapts itself to the case where no peak is found. If that is the case, it readjusts the distance threshold and also the height_threshold AUTOMATICALLY! This is indeed, a great improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak(interval,f,h_threshold,L_threshold,num_iterations, iteration_flag,hyperparameters):\n",
    "    '''\n",
    "    This function finds a peak of the function f in an interval with certain height and spatial \n",
    "    thresholds. It does not have to estimate the tresholds\n",
    "    \n",
    "    ----------------\n",
    "    Input:\n",
    "    - interval: which is a row vector of shape 1xm\n",
    "    - f: which is a row vector of shape 1xm\n",
    "    - h_threshold: which denotes the peak threshold\n",
    "    - L_threshold: an integer which denotes the plateau threshold\n",
    "    -iteration_flag : only used in the case that no peak is found. Whenever that happens, it increase the \n",
    "                       iteration flag by 1. The upper bound for iteration flag is 20 \n",
    "                        (hardcoded, but could be new hyperparameter)\n",
    "    ----------------\n",
    "    Output:\n",
    "    - locations[0]:  index of peak location (integer)\n",
    "    - interval[0,locations[0]]:  array value  of peak location (float)\n",
    "    - heights[0]:  intensity at peak location (float)\n",
    "    - num_iterations:  how many iterations were used.\n",
    "    ----------------\n",
    "    '''\n",
    "    \n",
    "    # Unpacking \n",
    "    iteration_decay = hyperparameters['iteration_decay']\n",
    "    f_maximal = rising_sun(f)\n",
    "    locations, heights = plateau_detection(f_maximal,L_threshold)\n",
    "    \n",
    "    # Now we detect the plateaus of f_maximal; the peak will be where the plateaus are located\n",
    "    \n",
    "    if (len(locations)==0): #locations==None):\n",
    "        print(\"In find_peaks: L_threshold is\", L_threshold,\"iteration_flag\", iteration_flag)\n",
    "        if L_threshold >2 and (iteration_flag<=20):  \n",
    "            if iteration_flag%2 ==0:\n",
    "                return find_peak(\n",
    "                interval,f,iteration_decay*h_threshold,L_threshold,num_iterations,iteration_flag+1,hyperparameters\n",
    "            )\n",
    "            \n",
    "            else:\n",
    "                return find_peak(\n",
    "                interval,f,h_threshold,L_threshold-1,num_iterations,iteration_flag+1,hyperparameters\n",
    "            )\n",
    "            '''\n",
    "       if L_threshold >2 and (iteration_flag<=10):       \n",
    "            return find_peak(\n",
    "                interval,f,iteration_decay*h_threshold,L_threshold-1,num_iterations,iteration_flag+1,hyperparameters\n",
    "            )\n",
    "            '''\n",
    "        else:\n",
    "            return (None,None,None,None)\n",
    "            \n",
    "    #now we  just need to check whether heigths is bigger than the threshold\n",
    "    # notice that we can always assume that if location has the 0 index, then heights is also 0. \n",
    "    #This is due to normalization\n",
    "    \n",
    "    # Now we retain only the elements that are higher than the threshold\n",
    "    what_matters= np.where(heights>=h_threshold)\n",
    "    locations, heights = locations[what_matters], heights[what_matters]\n",
    "    \n",
    "    if len(locations)==0: \n",
    "        print(\"In find_peaks: L_threshold is\", L_threshold,\"iteration_flag\", iteration_flag)\n",
    "        if L_threshold >2 and (iteration_flag<=20):  \n",
    "            if iteration_flag%2 ==0:\n",
    "                return find_peak(\n",
    "                interval,f,iteration_decay*h_threshold,L_threshold,num_iterations,iteration_flag+1,hyperparameters\n",
    "            )\n",
    "            \n",
    "            else:\n",
    "                return find_peak(\n",
    "                interval,f,h_threshold,L_threshold-1,num_iterations,iteration_flag+1,hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            return (None,None,None,None)\n",
    "        \n",
    "    else: return (locations[0], interval[0,locations[0]], heights[0],num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we put this all together in a function that will be used only once throughout the code, but in a very special role of finding the first peak and gathering a lot of information about the first peak, the estimated distance to previous peak and so. \n",
    "\n",
    "The next function has a distinct feature when compared to the find_peak: it takes into account that the 0th peak is probably the highest one (this is a very particular case of XANES, and is probably where the code for other types of Spectroscopy should be changed). It has two parameters that deal with that:\n",
    "\n",
    "    1.oscill_threshold: essentially, the parameter h_* as defined in the oscillation function equation shown above\n",
    "    2. first_peak_threshold: this is a threshold for the first peak that is given by the following formula\n",
    "    \n",
    "\\begin{align}\n",
    "\\textit{first_peak_threshold} = \\left( 1+ \\frac{\\text{total_oscillation}}{(\\lambda_5\\cdot \\text{oscill_threshold})}\\right)\\cdot \\text{oscill_threshold} \\tag{first_peak_threshold}.\\end{align}\n",
    "    \n",
    "The first term in red aims to give a relative proportion between the total_oscillation in the XANES curve (which reduces to 1 in the case of normalized XANES measurement; see function normalization presented above) and the noise it contains; the quantity $\\lambda_5$ is a hyperparameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_peak(\n",
    "    parameters,hyperparameters, name,rising_sun,toscillation, oscill_threshold,first_peak_threshold\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    This function finds the first peak of the function f in an interval with certain height and spatial \n",
    "    thresholds. Unlike \"find_peak\", it needs to estimate the thresholds\n",
    "    \n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - hyperparameters: hyperparamters dictionary\n",
    "    - name: name of material whose decay is being studied\n",
    "    - rising_sun\n",
    "    , oscill_threshold: height_threshold, as given by Def I.4 in the paper\n",
    "    - toscillation: oscillation function for current XANES measurement\n",
    "    - first_peak_threshold: see explanation in previous markdown\n",
    "    - rising_sun:  the Rising Sun function\n",
    "    - L_threshold: an integer which denotes the plateau threshold\n",
    "    -iteration_flag : only used in the case that no peak is found. Whenever that happens, it increase the \n",
    "                       iteration flag by 1. The upper bound for iteration flag is 20 \n",
    "                        (hardcoded, but could be new hyperparameter)\n",
    "    ----------------\n",
    "    Output:\n",
    "    \n",
    "    - tpeak_loc0:  index of peak location (integer)\n",
    "    - tpeak_energy0:  array value  of peak location (float)\n",
    "    - tpeak_height0, :  intensity at peak location (float)\n",
    "    - tnum_itera0:  how many iterations were used.\n",
    "    - tdist_peak:  distance to previous peak\n",
    "    - first_jump: jump in intensity (absolute value) when compared to previous peak\n",
    "    \n",
    "    ----------------\n",
    "    '''\n",
    "     # Unpacking\n",
    "    lambda_d_shrink_1st = hyperparameters['lambda_d_shrink_1st']\n",
    "    parameters_now = parameters[name]\n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "        \n",
    "    ### Initial estimate for the distance between peaks, which we will use to feed the algorithm\n",
    "    min_location, max_location = estimate_distance(parameters,hyperparameters,name,rising_sun,oscill_threshold)\n",
    "    tdist_peak= np.array([int(lambda_d_shrink_1st*(max_location- min_location))],ndmin=1)  ## IT WAS 1/2 before\n",
    "        \n",
    "    # Find peak locations:\n",
    "    iteration_flag=0\n",
    "    tpeak_loc0,tpeak_energy0,tpeak_height0,tnum_itera0 = \\\n",
    "    find_peak(E_now,rising_sun,first_peak_threshold,tdist_peak[-1],0,iteration_flag,hyperparameters)\n",
    "        \n",
    "    # Last, now that we really know where the first peak is, we update it:\n",
    "    tdist_peak=np.array([int(lambda_d_shrink_1st*(tpeak_loc0-min_location))]) ## IT WAS 1/2 before\n",
    "    first_jump = xanes_now[0,tpeak_loc0] - xanes_now[0,min_location]\n",
    "    \n",
    "    return (tpeak_loc0, tpeak_energy0, tpeak_height0, tnum_itera0, tdist_peak, first_jump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialy we just gave a crude estimate for the distanec between peaks and crests. However, once we reach the highest peak we can estimate again the distance between peaks\n",
    "\n",
    "What we are going to do it to find the fist maximum of the difference function and then use the data to extrapolate.\n",
    "\n",
    "\n",
    "First we write an auxiliary function to help us to print the data information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_brkpt_properties(j,peak_loc, peak_height, dist_peak, jumps):\n",
    "    '''\n",
    "    This function print the rpeakpoints properties:\n",
    "    \n",
    "    ----------------\n",
    "    Input:\n",
    "    - j: peak number\n",
    "    - peak_loc: location of peak\n",
    "    - peak_height: intensity of peak\n",
    "    - dist_peak: distance to previous peak\n",
    "    - jumps: intensity variation (in absolute value) with respect to previous peak\n",
    "    ----------------\n",
    "    Output:\n",
    "    None\n",
    "    ----------------\n",
    "    '''\n",
    "    print(\"\\t Peak \"+ str(j) +\" located at: \"+ str(peak_loc)) \n",
    "    print(\"\\t Peak height:\"+ str(peak_height) )    \n",
    "    print(\"\\t Distance between successive crests and peaks: \",dist_peak,\"\\n\")\n",
    "    print(\"\\t Jumps: \",jumps,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(August 12, 2019) At this point, we shall implement the hidden breakpoints method, that I discovered yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_breakpoint(xanes_now,left,middle,right,j):\n",
    "    '''\n",
    "    Implements hidden peak trick\n",
    "    ----------------\n",
    "    Input:\n",
    "    left<=middle<=right, three integer indexes\n",
    "    j: j checks wheter we need to look for a hidden crest or a hidden valley\n",
    "    ----------------\n",
    "    Output:\n",
    "    It depends on the input. \n",
    "        -If there is no need for it, it returns the midle.\n",
    "        - If there is need for it, it will return the minimum  of the vector \n",
    "    \n",
    "    ----------------\n",
    "    '''   \n",
    "    aux = xanes_now[0,left:right+1]\n",
    "    if j%2==0:  M= min(aux)\n",
    "    else: M= max(aux)\n",
    "        \n",
    "    if xanes_now[0,middle] ==M: ## In this case there is no need for the hidden peak trick\n",
    "        return middle\n",
    "    else: # in this case there was a problem, and we have to use the hidden peak trick\n",
    "        return left+np.min(np.where(M == aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__Remark:__ in the backstep move, the hidden peak trick can be iterated more than 2 times (in fact, it has to be iterated an even number of times). We took the minimal number of iterations by default, but a different upper bound on the number of iterations can be set as a hyperparameter.\n",
    "\n",
    "Now we define a dictionary with search conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_conditions = {\n",
    "    'move':'backstep',#'middle_step',\n",
    "    'printing':True,\n",
    "    'polite_guess':{},\n",
    "    'forward':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are finally ready to implement the main algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rising_sun_envelope_method(\n",
    "    parameters,hyperparameters, name, N_split_before,N_split_after,search_conditions\n",
    "):\n",
    "    '''\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - hyperparameters: hyperparamters dictionary\n",
    "    - name: name of material whose decay is being studied\n",
    "    - N_split_before:\n",
    "    - N_split_after:\n",
    "    - search_conditions: dictionary with search conditions:\n",
    "        - 'move':'backstep' or 'middle_step',\n",
    "        'printing': if middle computations should be printed\n",
    "        'polite_guess':{}, this is empty in the forward passing, but it is filled with first peak \n",
    "                        information in the reverse passing\n",
    "        'forward':True or False, denotes which passage of the algorithm we are, that is, whether\n",
    "                    looking fo peaks on the left of 0th peak or on the right\n",
    "    ----------------\n",
    "    Output:\n",
    "    A dictionary - decomposition-  with the following keys:\n",
    "\n",
    "    \n",
    "    - peak_loc: dictionary index of peak location (integer), counted from 0th peak (positive to right, negative to the left)\n",
    "    - peak_energy:array value  of peak location (float)\n",
    "    - peak_height: intensity at peak location (float)\n",
    "    - num_itera: how many iterations were used in inner loop that deals with peak not found\n",
    "    - dist_peak: vector with  distance between scuccessive peaks\n",
    "    - oscillation: oscillation function\n",
    "    - height_threshold: vector with the series of height thresholds\n",
    "    - jumps: jump in intensity (absolute value) when compared to previous peak\n",
    "    - number_split_before_peak: number of splits after 0th peak (including it)\n",
    "    - number_split_after_peak: number of splits after 0th peak (excluding it)\n",
    "    - oscil_jump_ratio: at jth >0 (resp. j<0) entry, \n",
    "                        ratio between XANES curve from from peak j to end of measurement (begining to jth peak)\n",
    "                        and the jump to previous (resp., next) peak\n",
    "    ----------------\n",
    "    '''\n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "    parameters_now= parameters[name]\n",
    "    stop = parameters_now['stop']    \n",
    "    xanes_now = parameters_now['xanes']\n",
    "    E_now =parameters_now['energy']\n",
    "    \n",
    "    # ...  hyperparameters\n",
    "    lambda_h = hyperparameters['lambda_h']\n",
    "    lambda_d = hyperparameters['lambda_d']\n",
    "    init_oscil_par = hyperparameters['initial_oscillation_guess_parameter']\n",
    "    \n",
    "    # ...  search condition parameters\n",
    "    move=search_conditions['move']\n",
    "    printing=search_conditions['printing']\n",
    "    polite_guess=search_conditions['polite_guess']\n",
    "    forward = search_conditions['forward']\n",
    "    ##############################################################\n",
    "    \n",
    "    if printing: \n",
    "        print(\"************************************** \\n\")\n",
    "        print(\"Material:\", name,\"\\n\")\n",
    "\n",
    "    start = 0\n",
    "    \n",
    "    # We shall save the resuls as dictionaries and vectors\n",
    "    peak_loc, peak_energy, peak_height, num_itera,height_threshold = {},{},{},{},{}\n",
    "    dist_peak,  jumps = [],[]\n",
    "        \n",
    "    ### Initializing some counters\n",
    "    number_split_after_peak, number_split_before_peak, iteration_flag=0,0,0\n",
    "    \n",
    "    ## For book keeping purposes\n",
    "    vector_peak_loc, vector_height_threshold =np.array([],dtype=np.int16), np.array([],dtype=np.float16)\n",
    "    \n",
    "    for j in range(max(N_split_after,1)):\n",
    "        if (j==0):\n",
    "            '''\n",
    "            REMARK: j =0 is a case that is only considered in the forward passing.\n",
    "                     It is necessary in order to find the first peak\n",
    "                     This first part can be used as a bootstrap in order to find the distance:\n",
    "            '''   \n",
    "            if forward==False:\n",
    "                '''\n",
    "                If false, then we are in the reverse case, which means that these quantities need to be initialized\n",
    "                '''\n",
    "                # First of all, we recover the parameters we are going to use\n",
    "                peak_loc[str(0)] = polite_guess['peak_loc_initial']\n",
    "                rising_sun_f= polite_guess['initial_rising_sunf_f']\n",
    "                height_threshold[str(0)]= polite_guess['initial_height_threshold']\n",
    "                oscillation = polite_guess['initial_oscillation']\n",
    "                critical_threshold= polite_guess['initial_critical_threshold']\n",
    "                dist_peak = polite_guess['dist_peak_name']\n",
    "                jumps = np.array([0])\n",
    "            \n",
    "                ### Book keeping\n",
    "                vector_peak_loc = np.append(vector_peak_loc,peak_loc[str(0)])\n",
    "                vector_height_threshold = np.append(vector_height_threshold,height_threshold[str(0)])\n",
    "                continue\n",
    "           \n",
    "            if printing: \n",
    "                print(\"************************************** \\n\")\n",
    "                print(\"\\t Going for \" +str(j)+\"th breakpoint. \\n\")\n",
    "            \n",
    "            rising_sun_f = rising_sun(xanes_now)        \n",
    "            \n",
    "            ### This is where the oscillation function gets calculated\n",
    "            _, oscillation, critical_threshold= oscillation_function(E_now,xanes_now)\n",
    "            \n",
    "            #This is where the compute the height threshold\n",
    "            height_threshold[\"0\"]=(int(oscillation[0,-1]/(init_oscil_par*critical_threshold)) + 1)*critical_threshold\n",
    "            \n",
    "            '''And this is where we look for the first peak\n",
    "               We remark that it is here that part of the hyperparameters are used, in the function\n",
    "               find_fisrst_peak > estimate_distance'''\n",
    "            peak_loc[\"0\"], peak_energy[\"0\"], peak_height[\"0\"],\\\n",
    "            num_itera[\"0\"], dist_peak, jumps=\\\n",
    "            find_first_peak(\n",
    "                parameters,hyperparameters, name,\\\n",
    "                rising_sun_f,oscillation, critical_threshold,\\\n",
    "                height_threshold[\"0\"]\n",
    "            )\n",
    "            \n",
    "            ### Book keeping\n",
    "            vector_peak_loc = np.append(vector_peak_loc,peak_loc[str(j)])\n",
    "            vector_height_threshold = np.append(vector_height_threshold,height_threshold[str(j)])\n",
    "            \n",
    "            if printing:  print_brkpt_properties(j,peak_loc[str(j)], peak_height[str(j)], dist_peak, jumps)\n",
    "                \n",
    "            number_split_after_peak +=1    \n",
    "            \n",
    "        #secondary cases - for which we have to use a maximal function\n",
    "        elif j%2==0:\n",
    "            if printing: \n",
    "                print(\"\\t ***************************** \\n\")\n",
    "                print(\"\\t Going for \" +str(j)+\"th breakpoint. \\n\")\n",
    "            start= peak_loc[str(j-1)]+1\n",
    "            interval = np.array(E_now[0,start:],ndmin=2)\n",
    "            \n",
    "            #Truncate to the appropriate interval\n",
    "            height_normalizer =  np.copy(xanes_now[0,start])\n",
    "            f = np.array(xanes_now[0,start:],ndmin=2) -height_normalizer       ## you always normalize\n",
    "            rising_sun_f = rising_sun(f)\n",
    "            \n",
    "            # Update the definition of the height threshold\n",
    "            decay_factor, temp,height_threshold[str(j)],_ =\\\n",
    "            decay_rate(\n",
    "                parameters,hyperparameters,name,oscillation, jumps,\\\n",
    "                vector_height_threshold,vector_peak_loc, dist_peak,j\n",
    "            )\n",
    "            tpeak_loc, tpeak_energy, tpeak_height, tnum_itera=\\\n",
    "            find_peak(\n",
    "                interval,f,height_threshold[str(j)],temp,0,iteration_flag,hyperparameters\n",
    "            )           \n",
    "            \n",
    "            if (tpeak_loc ==None) or (tpeak_loc+ start >=stop):  ### Then you can stop here!!!\n",
    "                if printing: \n",
    "                    print(\"\\n ONLY \"+ str(number_split_after_peak)+\" SPLITTINGS WERE POSSIBLE!\\n\") \n",
    "                break   \n",
    "                \n",
    "            peak_loc[str(j)], peak_energy[str(j)],peak_height[str(j)],num_itera[str(j)]=\\\n",
    "            tpeak_loc,tpeak_energy,tpeak_height,tnum_itera\n",
    "            \n",
    "            # We go back to the non-truncated vector\n",
    "            peak_loc[str(j)] = peak_loc[str(j)] +start\n",
    "            \n",
    "            ### Book keeping\n",
    "            vector_peak_loc = np.append(vector_peak_loc,peak_loc[str(j)])\n",
    "            vector_height_threshold = np.append(vector_height_threshold,height_threshold[str(j)])\n",
    "            \n",
    "            ##Now we need to check whether hidden peak trick is necessary\n",
    "            if j>=2:\n",
    "                p = hidden_breakpoint(xanes_now,peak_loc[str(j-2)],peak_loc[str(j-1)],peak_loc[str(j)],j)\n",
    "                ## MIDDLE MOVE\n",
    "                if (move== 'middle_step') or (peak_loc[str(j-1)]==p):\n",
    "                    peak_loc[str(j-1)]=p\n",
    "                    peak_energy[str(j-1)]=E_now[0,p]\n",
    "                    peak_height[str(j-1)]=xanes_now[0,p]\n",
    "                    dist_peak[-1] = int(peak_loc[str(j-1)]- peak_loc[str(j-2)])\n",
    "                \n",
    "                    ### Recompute these quantities\n",
    "                    start= peak_loc[str(j-1)]+1\n",
    "                    #Truncate to the appropriate interval\n",
    "                    height_normalizer =  np.copy(xanes_now[0,start])\n",
    "                    f = np.array(xanes_now[0,start:],ndmin=2) -height_normalizer       ## you always normalize\n",
    "                    rising_sun_f = rising_sun(f)\n",
    "                    \n",
    "                    # Recalculate decay factor\n",
    "                    vector_peak_loc[j-1] =peak_loc[str(j-1)]\n",
    "                    decay_factor, _,height_threshold[str(j)],_ =\\\n",
    "                    decay_rate(\n",
    "                        parameters,hyperparameters,name,oscillation, jumps,vector_height_threshold,vector_peak_loc, dist_peak,j\n",
    "                    )\n",
    "                    jumps[-1] =  np.abs(xanes_now[0,peak_loc[str(j-1)]] - xanes_now[0,peak_loc[str(j-2)]])\n",
    "                               \n",
    "                else: ## BACKSTEP MOVE\n",
    "                    ## Can be iterated, but we won't go for it\n",
    "                    p = hidden_breakpoint(xanes_now,peak_loc[str(j-1)],p,p,j-1)\n",
    "                    \n",
    "                    peak_loc[str(j)]=p\n",
    "                    ### we don't need to recompute the quantities f and rising_sun_f, just the jump and others\n",
    "                    peak_energy[str(j)]=E_now[0,p]\n",
    "                    \n",
    "            #############################################################           \n",
    "            #upgrade the dist_peak matrix\n",
    "            \n",
    "            dist_peak = \\\n",
    "            np.append(dist_peak,np.array([int(peak_loc[str(j)]- peak_loc[str(j-1)])]) ) \n",
    "            \n",
    "            # unnormalize peak height\n",
    "            peak_height[str(j)] = xanes_now[0,peak_loc[str(j)]]#==peak_height[str(j)]+height_normalizer\n",
    "                                 \n",
    "            # Book keeping    \n",
    "            jumps = np.append(jumps, np.abs(xanes_now[0,peak_loc[str(j)]] - xanes_now[0,peak_loc[str(j-1)]]))\n",
    "            \n",
    "            if printing: \n",
    "                print_brkpt_properties(j,peak_loc[str(j)], peak_height[str(j)], dist_peak, jumps)\n",
    "            \n",
    "            number_split_after_peak +=1    \n",
    "            \n",
    "        ## third case, for which we use the valley_of_shadows function\n",
    "        \n",
    "        elif j%2 ==1:   \n",
    "            if printing: \n",
    "                print(\"\\t ***************************** \\n\")\n",
    "                print(\"\\t Going for \" +str(j)+\"th breakpoint. \\n\")\n",
    "            # First: set up the interval of relevance\n",
    "            start= peak_loc[str(j-1)]+1\n",
    "            interval = np.array(E_now[0,start:],ndmin=2)\n",
    "            \n",
    "            #At this point we do the following: \n",
    "            #1) We truncate the maximal function from start to end\n",
    "            #2) Construct first difference\n",
    "            rel_start = start\n",
    "            if j>1: rel_start -= (peak_loc[str(j-2)]+1)\n",
    "            rising_sun_f = np.array(rising_sun_f[0,rel_start:],ndmin=2)\n",
    "            f = np.array(xanes_now[0,start:],ndmin=2)\n",
    "            valley_of_shadows = rising_sun_f- f #this function will always be nonnegative, thanks to (1)  \n",
    "            \n",
    "            ## Now we renormalize, because it has to start from 0\n",
    "            valley_of_shadows = valley_of_shadows- valley_of_shadows[0,0] \n",
    "            f = np.array(valley_of_shadows,ndmin=2)\n",
    "            \n",
    "            ## UPDATES...\n",
    "            #... of height threshold\n",
    "            # Update the definition of the height threshold\n",
    "            decay_factor, temp,height_threshold[str(j)],_ =\\\n",
    "            decay_rate(\n",
    "                parameters,hyperparameters,name,oscillation, jumps,\\\n",
    "                vector_height_threshold,vector_peak_loc, dist_peak,j\n",
    "            )\n",
    "            \n",
    "            tpeak_loc, tpeak_energy, tpeak_height, tnum_itera=\\\n",
    "            find_peak(interval,f,height_threshold[str(j)],temp,0,iteration_flag,hyperparameters)           \n",
    "            \n",
    "            if (tpeak_loc ==None) or (tpeak_loc+ start >=stop):  ### Then you can stop here!!!\n",
    "                if printing: \n",
    "                    print(\"\\n ONLY \"+ str(number_split_after_peak)+\" SPLITTINGS WERE POSSIBLE!\\n\") \n",
    "                break   \n",
    "                \n",
    "            peak_loc[str(j)], peak_energy[str(j)],peak_height[str(j)],num_itera[str(j)]=\\\n",
    "            tpeak_loc,tpeak_energy,tpeak_height,tnum_itera\n",
    "            \n",
    "            # We go back to the non-truncated vector\n",
    "            peak_loc[str(j)] = peak_loc[str(j)]+ start\n",
    "            \n",
    "            ### Book keeping\n",
    "            vector_peak_loc = np.append(vector_peak_loc,peak_loc[str(j)])\n",
    "            vector_height_threshold = np.append(vector_height_threshold,height_threshold[str(j)])\n",
    "            \n",
    "            ##############################################################\n",
    "            ##Now we need to check whether hidden peak trick is necessary\n",
    "                \n",
    "            if j>=2:\n",
    "                p = hidden_breakpoint(xanes_now,peak_loc[str(j-2)],peak_loc[str(j-1)],peak_loc[str(j)],j)\n",
    "            \n",
    "                if (move== 'middle_step') or (peak_loc[str(j-1)]==p):\n",
    "                    peak_loc[str(j-1)]=p\n",
    "                    peak_energy[str(j-1)]=E_now[0,p]\n",
    "                    peak_height[str(j-1)]=xanes_now[0,p]\n",
    "                    dist_peak[-1] = int(peak_loc[str(j-1)]- peak_loc[str(j-2)])\n",
    "                \n",
    "                    ### Recompute these quantities\n",
    "                    start= peak_loc[str(j-1)]+1\n",
    "                    #Truncate to the appropriate interval\n",
    "                    height_normalizer =  np.copy(xanes_now[0,start])\n",
    "                    f = np.array(xanes_now[0,start:],ndmin=2) -height_normalizer       ## you always normalize\n",
    "                    rising_sun_f = rising_sun(f)\n",
    "                \n",
    "                    # Recalculate decay factor\n",
    "                    vector_peak_loc[j-1] =peak_loc[str(j-1)]\n",
    "                    #vector_height_threshold[j]= height_threshold[str(j)] \n",
    "                    decay_factor,_,height_threshold[str(j)],_ =\\\n",
    "                    decay_rate(\n",
    "                        parameters,hyperparameters,name,oscillation, jumps,vector_height_threshold,vector_peak_loc, dist_peak,j\n",
    "                    )\n",
    "                    jumps[-1] =  np.abs(xanes_now[0,peak_loc[str(j-1)]] - xanes_now[0,peak_loc[str(j-2)]])\n",
    "                                        \n",
    "                else: ## BACKSTEP MOVE\n",
    "                    ## Can be iterated, but we will iterate only once\n",
    "                    p = hidden_breakpoint(xanes_now,peak_loc[str(j-1)],p,p,j-1)\n",
    "                    \n",
    "                    peak_loc[str(j)]=p\n",
    "                    ### we don't need to recompute the quantities f and rising_sun_f, just the jump and others\n",
    "                    peak_energy[str(j)]=E_now[0,p]\n",
    "                    \n",
    "            ##############################################################\n",
    "            #upgrade the dist_peak matrix\n",
    "            dist_peak = \\\n",
    "            np.append(dist_peak,np.array([int((peak_loc[str(j)]- peak_loc[str(j-1)]))]) )        \n",
    "           \n",
    "            peak_height[str(j)] =xanes_now[0,peak_loc[str(j)]]        \n",
    "            jumps = np.append(jumps, np.abs(xanes_now[0,peak_loc[str(j)]] - xanes_now[0,peak_loc[str(j-1)]]))\n",
    "           \n",
    "            if printing: \n",
    "                print_brkpt_properties(j,peak_loc[str(j)], peak_height[str(j)], dist_peak, jumps)\n",
    "            \n",
    "            number_split_after_peak +=1\n",
    "            \n",
    "    # In this part we figure out whether the algorithm goes for a second round of measurements, but now using \n",
    "    # the previous estimates on distance to improve the accuracy of this estimate\n",
    "    \n",
    "    ## Compute oscillation metric:\n",
    "    oscil_jump_ratio ={}\n",
    "    x, y = np.array(E_now[0,peak_loc[\"0\"]:stop],ndmin=2),np.array(xanes_now[0,peak_loc[\"0\"]:stop],ndmin=2)\n",
    "    _,_, amount_oscil = oscillation_function(x,y, full_computation=False)\n",
    "    oscil_jump_ratio[\"0\"] =  amount_oscil/jumps[0]\n",
    "    \n",
    "    ##################\n",
    "    # We need to do this because the algorithm is asymetric\n",
    "    flag_oscillation =1\n",
    "    if forward:\n",
    "        flag_oscillation =0\n",
    "        \n",
    "    for i in range(1,number_split_after_peak +flag_oscillation):\n",
    "        x, y = np.array(E_now[0,peak_loc[str(i)]:],ndmin=2),np.array(xanes_now[0,peak_loc[str(i)]:],ndmin=2)\n",
    "        _,_, amount_oscil = oscillation_function(x,y, full_computation=False)\n",
    "        oscil_jump_ratio[str(i)] =  amount_oscil/jumps[i]\n",
    "        if np.isnan(oscil_jump_ratio[str(i)]):\n",
    "            oscil_jump_ratio[str(i)] = np.inf\n",
    "    ##################\n",
    "    \n",
    "    \n",
    "    if N_split_before==0:\n",
    "        if printing: \n",
    "            print(\"\\n ONLY \"+ str(number_split_after_peak+number_split_before_peak)+\" SPLITTINGS \\\n",
    "            TO THE RIGHT OF THE MAIN PEAK WERE POSSIBLE!\") \n",
    "        \n",
    "        decomposition={\n",
    "            'peak_location':peak_loc,'peak_energy':peak_energy,\n",
    "            'peak_heights':peak_height,'number_iterations':num_itera,\n",
    "            'distance_between_peaks':dist_peak, 'oscillation':oscillation,\n",
    "            'height_threshold_evolution':height_threshold,'jumps':jumps,\n",
    "            'number_of_splittings_before':number_split_before_peak, \n",
    "            'number_of_splittings_after':number_split_after_peak,\n",
    "            'oscil_jump_ratio':oscil_jump_ratio\n",
    "        }\n",
    "        return decomposition\n",
    "        \n",
    "    ###############################################################################################      \n",
    "    ### REVERSE PASSING:\n",
    "    ### we reverse the spectra to do the search on the other side of the mountain\n",
    "    ### At this point, the algorithm is using the previous data to improve the location of the peaks    \n",
    "    ###############################################################################################      \n",
    "    \n",
    "    if printing: \n",
    "        print(\"************************************** \\n\")\n",
    "        print(\"\\t STARTING REVERSE PASSING, ON THE LEFT SIDE\\n\")\n",
    "    # We want to include the peak_loc(0)\n",
    "    chopped_xanes = np.array(xanes_now[0,:peak_loc[\"0\"]+1],ndmin=2)\n",
    "    chopped_E = np.array(E_now[0,:peak_loc[\"0\"]+1],ndmin=2)\n",
    "    \n",
    "    ## Now we reverse these vectors\n",
    "    chopped_xanes = np.array(chopped_xanes[0,::-1],ndmin=2)\n",
    "    chopped_E = np.array(chopped_E[0,::-1],ndmin=2)\n",
    "    \n",
    "    ### REMARK: THE ABOVE TWO OPERATIONS ARE NOT COMMUTATIVE!!!\n",
    "    \n",
    "    if number_split_after_peak >1: # Then we can remove the first element\n",
    "        dist_peak = dist_peak[1:]  # REMOVE FIRST ELEMENT!!!\n",
    "        jumps = jumps[1:]  # REMOVE FIRST ELEMENT!!!\n",
    "    \n",
    "    ## And then we pad these vectors\n",
    "    reverse_E,reverse_xanes, reverse_stop = \\\n",
    "    small_padded_spectra(chopped_E,chopped_xanes)\n",
    "    \n",
    "    # Things that we are going to use in the REVERSE PASSING\n",
    "    # ... parameters\n",
    "    reverse_N_split_after= N_split_before+1  # add 1 because we are not looking for 0 anymore\n",
    "    reverse_N_split_before=0\n",
    "    \n",
    "    ##################################################################\n",
    "    #Let's create a new parameters dictionary\"\n",
    "    reverse_parameters ={\n",
    "        name:{'name':name,\n",
    "              'xanes':reverse_xanes,\n",
    "              'energy':reverse_E,\n",
    "              'stop':reverse_stop }\n",
    "    }\n",
    "    \"...and a new hyperparameters dictionary\"\n",
    "    reverse_hyperparameters={\n",
    "         ## These hyperparameters will be used in \n",
    "        'lambda_h':1,'lambda_d':1/4,\n",
    "        ## These hyperparameters will be used in find_first_peak\n",
    "        'lambda_h_find_1st':4,'lambda_d_find_1st':1/4,\n",
    "        #'lambda_d_shrink_1st':1/2, # WON'T BE NEEDED\n",
    "        'initial_oscillation_guess_parameter':10,\n",
    "      ## These hyperparameters will be used in \n",
    "        'stretching_factor':3,'iteration_decay':.9,'decay_rate_type': 'reversed'\n",
    "    }\n",
    "    # ...  search condition parameters. But before we do that we need to update a few things\n",
    "    reverse_polite_guess={\n",
    "        'peak_loc_initial':int(0),\n",
    "        'initial_rising_sunf_f': rising_sun(reverse_xanes),\n",
    "        'initial_height_threshold': height_threshold[str(0)],\n",
    "        'initial_oscillation':oscillation,\n",
    "        'initial_critical_threshold':critical_threshold,\n",
    "        'dist_peak_name': int(np.average(dist_peak/4)),\n",
    "        'jump': np.average(jumps/4)\n",
    "    }\n",
    "                                        \n",
    "    reverse_search_conditions = {\n",
    "        'move':'middle_step','printing':False,\n",
    "        'polite_guess':reverse_polite_guess,'forward':False\n",
    "    }\n",
    "    ##################################################################\n",
    "    \n",
    "    # Then we call the same algorithm on this reversed spectrum.\n",
    "    \n",
    "    reverse_decomposition= \\\n",
    "    rising_sun_envelope_method(\n",
    "        reverse_parameters,reverse_hyperparameters,name,\\\n",
    "        reverse_N_split_before, reverse_N_split_after,reverse_search_conditions\n",
    "    )\n",
    "    \n",
    "    # Now we unpack the result\n",
    "    reverse_peak_loc = reverse_decomposition['peak_location']  \n",
    "    reverse_peak_energy = reverse_decomposition['peak_energy'] \n",
    "    reverse_peak_height = reverse_decomposition['peak_heights'] \n",
    "    reverse_num_itera = reverse_decomposition['number_iterations']  \n",
    "    reverse_dist_peak = reverse_decomposition['distance_between_peaks']   \n",
    "    reverse_oscillation = reverse_decomposition['oscillation'] \n",
    "    reverse_height_threshold = reverse_decomposition['height_threshold_evolution'] \n",
    "    reverse_number_splittings_before = reverse_decomposition['number_of_splittings_before']\n",
    "    reverse_number_splittings_after = reverse_decomposition['number_of_splittings_after']\n",
    "    reverse_jumps = reverse_decomposition['jumps'] \n",
    "    reverse_oscil_jump_ratio = reverse_decomposition['oscil_jump_ratio'] \n",
    "    \n",
    "    #And in the end we need to put things together.\n",
    "    # The terms don't need reflection, for they are invariant\n",
    "    \n",
    "    if number_split_after_peak >1: # Then we can remove the first element\n",
    "        reverse_dist_peak = reverse_dist_peak[1:]\n",
    "        reverse_dist_peak = reverse_dist_peak[::-1]\n",
    "        dist_peak = np.append(reverse_dist_peak,dist_peak)                  \n",
    "        \n",
    "        reverse_jumps = reverse_jumps[1:]\n",
    "        reverse_jumps = reverse_jumps[::-1]\n",
    "        jumps = np.append(reverse_jumps,jumps)\n",
    "    else:\n",
    "        reverse_jumps = reverse_jumps[1:]\n",
    "        jumps = reverse_jumps[::-1]\n",
    "        \n",
    "        reverse_dist_peak = reverse_dist_peak[1:]\n",
    "        dist_peak = reverse_dist_peak[::-1]\n",
    "        \n",
    "    number_split_before_peak = reverse_number_splittings_after\n",
    "    for k in range(1,number_split_before_peak+1):\n",
    "        # unnormalize peak height\n",
    "        peak_loc[str(-k)] =peak_loc[\"0\"] - (reverse_peak_loc[str(k)])\n",
    "        peak_energy[str(-k)] = reverse_peak_energy[str(k)]\n",
    "        peak_height[str(-k)] = reverse_peak_height[str(k)]\n",
    "        num_itera[str(-k)]  = reverse_num_itera[str(k)]  \n",
    "        height_threshold[str(-k)] = reverse_height_threshold[str(k)]   \n",
    "        oscil_jump_ratio[str(-k)] = reverse_oscil_jump_ratio[str(k)]\n",
    "    \n",
    "    if printing: \n",
    "        print(\"\\n ONLY \"+ str(number_split_before_peak)+\" SPLITTINGS \\\n",
    "            TO THE LEFT OF THE MAIN PEAK WERE POSSIBLE!\\n\\n\") \n",
    "        print(\"\\n In TOTAL, ONLY \"+ str(number_split_after_peak+number_split_before_peak)+\" SPLITTINGS WERE POSSIBLE!\\n\") \n",
    "    \n",
    "    decomposition={\n",
    "        'peak_location':peak_loc,'peak_energy':peak_energy,\n",
    "        'peak_heights':peak_height,'number_iterations':num_itera,\n",
    "        'distance_between_peaks':dist_peak, 'oscillation':oscillation,\n",
    "        'height_threshold_evolution':height_threshold,'jumps':jumps,\n",
    "        'number_of_splittings_before':number_split_before_peak, \n",
    "        'number_of_splittings_after':number_split_after_peak,\n",
    "        'oscil_jump_ratio':oscil_jump_ratio\n",
    "    } \n",
    "    return decomposition    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_conditions['printing'] = False\n",
    "hyperparameters['lambda_d']=1/4\n",
    "hyperparameters['decay_rate_type']= 'min_max'#'learn_to_trust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def write_material_peak_properties(\n",
    "    parameters,hyperparameters,search_conditions,\\\n",
    "    names, N_split_after, N_split_before\n",
    "):\n",
    "    '''\n",
    "       ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - hyperparameters: hyperparamters dictionary\n",
    "    - names: names of materials studied\n",
    "    - N_split_before: upper bound on number of peaks to the right (always set to be greater than 1,\n",
    "                      because 0th peak is included) \n",
    "    - N_split_after:upper bound on number of peaks to the left \n",
    "    - search_conditions: dictionary with search conditions:\n",
    "        - 'move':'backstep' or 'middle_step',\n",
    "        'printing': if middle computations should be printed\n",
    "        'polite_guess':{}, this is empty in the forward passing, but it is filled with first peak \n",
    "                        information in the reverse passing\n",
    "        'forward':True or False, denotes which passage of the algorithm we are, that is, whether\n",
    "                    looking fo peaks on the left of 0th peak or on the right\n",
    "    ----------------\n",
    "    Output:\n",
    "    - decompositions is a dictionary with names elements as keys, and elements are decomposition dictionary. \n",
    "    Each  decomposition dictionary has the  following keys:\n",
    "\n",
    "    - peak_loc: dictionary index of peak location (integer), counted from 0th peak (positive to right, negative to the left)\n",
    "    - peak_energy:array value  of peak location (float)\n",
    "    - peak_height: intensity at peak location (float)\n",
    "    - num_itera: how many iterations were used in inner loop that deals with peak not found\n",
    "    - dist_peak: vector with  distance between scuccessive peaks\n",
    "    - oscillation: oscillation function\n",
    "    - height_threshold: vector with the series of height thresholds\n",
    "    - jumps: jump in intensity (absolute value) when compared to previous peak\n",
    "    - number_split_before_peak: number of splits after 0th peak (including it)\n",
    "    - number_split_after_peak: number of splits after 0th peak (excluding it)\n",
    "    - oscil_jump_ratio: at jth >0 (resp. j<0) entry, \n",
    "                        ratio between XANES curve from from peak j to end of measurement (begining to jth peak)\n",
    "                        and the jump to previous (resp., next) peak\n",
    "    '''\n",
    "    decompositions={}\n",
    "    ## We are going to split in N_split features\n",
    "    for name in names:\n",
    "        decompositions[name]=\\\n",
    "        rising_sun_envelope_method(\n",
    "            parameters,hyperparameters,name, N_split_before, N_split_after,search_conditions\n",
    "        )\n",
    " \n",
    "    return decompositions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_split_after, N_split_before=30, 5\n",
    "\n",
    "decompositions =\\\n",
    "write_material_peak_properties(\n",
    "    parameters,hyperparameters,search_conditions,names,N_split_after , N_split_before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"decompositions\" is a dictionary of dictionaries, and its keys are the names of each material. In each one of these sub-dictionaries we have many properties of each decomposition, as we show next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = names[0]\n",
    "decompositions[name].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of decompositions is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(\n",
    "        name,\"has\",decompositions[name]['number_of_splittings_before'],\" splittings before, and\",\\\n",
    "        decompositions[name]['number_of_splittings_after'],\"splittings after\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_marked_peak_spectrum(\n",
    "    parameters,names,decompositions,\\\n",
    "    save_as,plot_type='normalized',\\\n",
    "    figure_extension=\"eps\",loc=4,title = \"XANES spectrum with marked peaks\"\n",
    "):\n",
    "    '''\n",
    "    This function plots the spectra in the dictionary parameters that are indexed by the elements in names, \n",
    "    and mark them with the peaks given in the dictionary decompositions\n",
    "    The plots are saved with name save_as.\n",
    "    Inputs can be normalized or not ('raw' case)\n",
    "    \n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - plot_type: 'normalized' or 'raw'\n",
    "    - figure_extension: 'png' or 'eps'\n",
    "    - loc: number from {1,2, 3, 4,} where legend will be placed\n",
    "    - title: title in the plot\n",
    "    ----------------\n",
    "    '''\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.rc('font', family='serif')\n",
    "    plt.rc('xtick', labelsize=18)\n",
    "    plt.rc('ytick', labelsize=18)\n",
    "\n",
    "    i = 0\n",
    "    for name in names:\n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "        # Unpacking\n",
    "        stop = parameters[name]['stop']    \n",
    "        if plot_type =='normalized':\n",
    "            xanes_now = parameters[name]['xanes']\n",
    "        elif plot_type =='raw':\n",
    "            xanes_now = parameters[name]['raw_xanes']\n",
    "        E_now =parameters[name]['energy']\n",
    "        decomposition_now= decompositions[name]\n",
    "        number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "        number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "        N = number_splittings_before+number_splittings_after\n",
    "    \n",
    "        plt.plot(\n",
    "            E_now[0,0:stop].T,xanes_now[0,0:stop].T,\\\n",
    "            label =name+\",#Breakpoints: \"+str(N),color='C'+str(i%10),lw=3,linestyle='-',alpha=.7\n",
    "        )\n",
    "        plt.grid(True)\n",
    "        i+=1\n",
    "        \n",
    "        # Plot a marker\n",
    "        colors = ['g','y','r']\n",
    "     \n",
    "        proportion_noise_jump = decomposition_now['oscil_jump_ratio']\n",
    "        values = [float(x) for x in list(proportion_noise_jump.values())]\n",
    "        for i in range(len(values)):\n",
    "            if np.isinf(values[i]):\n",
    "                values[i] = 3.0\n",
    "        \n",
    "        for plots_iter in range(-number_splittings_before,number_splittings_after):\n",
    "           # First peak marker\n",
    "            position = decomposition_now['peak_location'][str(plots_iter)]\n",
    "            plt.plot(\n",
    "                E_now[0,position],xanes_now[0,position],\\\n",
    "                color=colors[min(int(np.floor(values[plots_iter])),2)], \\\n",
    "                label=None,marker='H',lw=4, markersize=12,alpha=.8\n",
    "            )\n",
    "               \n",
    "    plt.title(title,size=28)\n",
    "    plt.legend(loc=loc,prop={'size': 22})\n",
    "    plt.xlabel('Energy (eV)',size=28)\n",
    "    plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "    figure_save(save_as,  figure_extension= figure_extension) \n",
    "    plt.show()    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marked_peak_spectrum(\n",
    "    parameters,names,decompositions,\\\n",
    "    save_as=\"XANES_with_marked_peaks_average-middle_step\",\\\n",
    "    plot_type='normalized'#'raw'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to visualize in the normalized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marked_peak_spectrum(\n",
    "    parameters,names,decompositions,\\\n",
    "    save_as=\"XANES_with_marked_peaks_average-middle_step\",\\\n",
    "    plot_type='normalized',\n",
    "    figure_extension=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in the original, non normalized spectrum, corresponds to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marked_peak_spectrum(\n",
    "    parameters,{names[3]},decompositions,\"raw_XANES_with_marked_peaks_average\",plot_type=\"raw\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decay rate plots for the regression method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_iter ={'1':'blue','2':'cyan','4':'purple'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "##############################################################\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "name = names[0]\n",
    "stop = parameters[name]['stop']\n",
    "E_now = parameters[name]['energy']\n",
    "xanes_now = parameters[name]['xanes']\n",
    "\n",
    "# ...decomposition properties\n",
    "decomposition_now = decompositions[name]\n",
    "peak_height = decomposition_now['peak_heights']\n",
    "vector_dist_peak = decomposition_now['distance_between_peaks']\n",
    "height_threshold = decomposition_now['height_threshold_evolution']\n",
    "jumps = decomposition_now['jumps']\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "omega_oscillation = decomposition_now['oscillation']\n",
    "        \n",
    "vector_height_threshold= [height_threshold[str(j)] for j in range(0,number_splittings_after)]\n",
    "vector_peak_loc = [peak_loc[str(j)] for j in range(0,number_splittings_after)]\n",
    "\n",
    "##Redefine stop\n",
    "stop =vector_peak_loc[-1]\n",
    "k = 15 ### Always an integer >= 1\n",
    "\n",
    "vector_dist_peak_after= vector_dist_peak[number_splittings_before:number_splittings_before+k]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.plot(E_now[0,:stop].T,xanes_now[0,:stop].T,color='blue',lw=4,linestyle='-',label='XANES: '+name,alpha=1)\n",
    "\n",
    "hyperparameters['decay_rate_type']= 'regression'\n",
    "\n",
    "_ , _ ,d , decay_plot =decay_rate(\n",
    "    parameters,hyperparameters,name,\\\n",
    "    omega_oscillation, jumps,vector_height_threshold,\\\n",
    "    vector_peak_loc, vector_dist_peak_after,k\n",
    ")\n",
    "\n",
    "# Unpacking\n",
    "pred  = decay_plot['range_xanes_pred']\n",
    "E_for_pred = decay_plot['range_energy_pred']\n",
    "error= decay_plot['error']\n",
    "colors= ['red','orange','purple','green','black']\n",
    "'''\n",
    "for k in range(4):\n",
    "\n",
    "    plt.plot(E_for_pred,(pred+np.power(2,k)*error),color=colors[k],linestyle='-.',lw=3,alpha=.3+.1*int(k))\n",
    "    plt.plot(E_for_pred,(pred-np.power(2,k)*error),color=colors[k],linestyle='-.',lw=3,label=str(k)+'band',alpha=.3+.1*int(k))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "'''\n",
    "l=1\n",
    "stretching_factor=3\n",
    "plt.plot(E_for_pred,(pred+stretching_factor*error),color='purple',linestyle=(0,(5,1)),lw=4,alpha=.3+.1*int(l))\n",
    "plt.plot(E_for_pred,(pred-stretching_factor*error),color='purple',linestyle=(0,(5,1)),lw=4,label=None,alpha=.3+.1*int(l))\n",
    "\n",
    "\n",
    "# On the left\n",
    "plt.plot([E_for_pred[0],\\\n",
    "            E_for_pred[0]],[pred[0,0]-stretching_factor*error[0,0],pred[0,0]+stretching_factor*error[0,0]],'-',\\\n",
    "         lw=4,color='red')\n",
    "# On the right\n",
    "plt.plot([E_for_pred[-1],\\\n",
    "            E_for_pred[-1]],[pred[-1,0]-stretching_factor*error[-1,0],pred[-1,0]+stretching_factor*error[-1,0]],'-',\\\n",
    "         lw=4,color='red')\n",
    "\n",
    "plt.legend(loc=4,prop={'size':22}) \n",
    "plt.title('Decay estimates using the regression method',size=28)\n",
    "plt.ylabel(r\" Absorption $\\mu(E)$\",size=28)\n",
    "plt.xlabel('Energy (eV)',size=28)\n",
    "\n",
    "plt.grid(True)    \n",
    "figure_save('Studying_decay_regression_method',figure_extension='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics on decay rates about the main peak\n",
    "\n",
    "As said before, the decay rate does not need to be of the type\n",
    "\n",
    "$$\\alpha_{\\text{decay}}^{(i)} = \\frac{\\max_{e_i\\leq E,F\\leq e_{\\infty}}\\vert\\mu(E)- \\mu(F)\\vert}{\\max_{e_{-\\infty}\\leq E,F\\leq e_{\\infty}}\\vert\\mu(E)-\\mu(F)\\vert}$$\n",
    "\n",
    "and in this section we study some other possible decay rates using regression\n",
    "\n",
    "\n",
    "We are going to compare the decay rates using the parameter above with that using the peak heights decay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## To create my own legend\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "## To avoid all the annoying deprecated warnings in matplotlib\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Statistics with jumps\n",
    "\n",
    "We shall prot the graph and the jumps right below it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_plot_jumps(name,parameters,decompositions,number_points,saveas='Statistics',plotting=True):\n",
    "\n",
    "    '''\n",
    "    This function plots the XANES spectrum with marked peaks. In a second plot, at the position of a peak j \n",
    "    it plots the jump (absolute value of difference in  intensity) between the value of absorption at the peak j\n",
    "    to that of peak j-1\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - number_points: number of peaks that we want to do statistics on\n",
    "    - saveas:  name  of saved figure\n",
    "    - plotting: plot figure (True) or not  (False)\n",
    "    ----------------\n",
    "    Output:\n",
    "    ----------------\n",
    "    - \n",
    "    '''\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize=18)\n",
    "        plt.rc('ytick', labelsize=18)\n",
    "    \n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "    print(\"Statistics for\", name,\"\\n\")\n",
    "    parameters_now = parameters[name]\n",
    "    stop = parameters_now['stop']\n",
    "    E_now = parameters_now['energy']\n",
    "    xanes_now = parameters_now['xanes']\n",
    "\n",
    "    # ...decomposition properties\n",
    "    decomposition_now = decompositions[name]\n",
    "    peak_height = decomposition_now['peak_heights']\n",
    "    vector_dist_peak = decomposition_now['distance_between_peaks']\n",
    "    height_threshold = decomposition_now['height_threshold_evolution']\n",
    "    jumps = decomposition_now['jumps']\n",
    "    peak_loc = decomposition_now['peak_location']\n",
    "    number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "    number_points = min(decomposition_now['number_of_splittings_after'], number_points)\n",
    "    omega_oscillation = decomposition_now['oscillation']\n",
    "    \n",
    "    # New quantities\n",
    "    alpha_before = np.zeros(number_splittings_before)  # We shall put tehe alpha values here\n",
    "    jumps_before =jumps[0:number_splittings_before]\n",
    "    alpha_after = np.zeros(number_points-1)    # We shall put tehe alpha values here\n",
    "    jumps_after = jumps[number_splittings_before-1:]\n",
    "    \n",
    "    #To the right of the main peak\n",
    "    vector_height_threshold_used= [height_threshold[str(j)] for j in range(0,number_points)]\n",
    "    '''\n",
    "    We shall plot the xanes measurement and the peak variations'''\n",
    "    f = plt.figure(figsize=(15,7))\n",
    "    ax1 = plt.subplot(111)\n",
    "   \n",
    "    where_to_stop_plot = int(.8*peak_loc[str(number_points-1)]+.2*stop)\n",
    "    ax1.plot(E_now[0,:where_to_stop_plot].T,xanes_now[0,:where_to_stop_plot].T,color='blue',lw=4) # Plot xanes spectrum \n",
    "    ax1.grid(True)\n",
    "    e =  [E_now[0,peak_loc[str(0)]]]\n",
    "    vector_height_threshold= []\n",
    "              \n",
    "    start_from =0\n",
    "    \n",
    "    for i in range(start_from,number_points-1):\n",
    "        \n",
    "        alpha_after[i] =\\\n",
    "        np.max(xanes_now[0,peak_loc[str(i)]:])-np.min(xanes_now[0,peak_loc[str(i)]:])/omega_oscillation[0,-1]\n",
    "        plt.subplot(111)\n",
    "    \n",
    "        _,_,h_threshold =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "    ############################\n",
    "        ### How much of the jump is noise?\n",
    "        proportion_noise_jump=h_threshold/jumps_after[i]\n",
    "        colors = ['g','y','r']\n",
    "     \n",
    "    # Plot a marker\n",
    "        ax1.plot(E_now[0,peak_loc[str(i)]],xanes_now[0,peak_loc[str(i)]],\\\n",
    "                 color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='H',label=None,  markersize=16,alpha=.8,lw=4)\n",
    "        \n",
    "        # 'amount of variation from this point on\n",
    "        ax1.bar(E_now[0,peak_loc[str(i)]],jumps_after[i],width=4,color='red')\n",
    "        plt.title('Jumps with respect to previous breakpoint')\n",
    "        plt.grid(True)\n",
    "    #########################################\n",
    "    ## LEGENDS...\n",
    "    red_patch = mpatches.Patch(color='red',alpha=.4, label='Relative jump')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='xanes')\n",
    "\n",
    "    ax1.legend(handles=[red_patch, blue_patch],prop={'size':22})\n",
    "    plt.xlabel('Energy (eV)',size=22)\n",
    "    plt.ylabel('Absorption $\\mu(E)$ (Normalized)',size=22)\n",
    "    plt.title(\"Study of decay rates\",size=28)\n",
    "    figure_save(saveas,figure_extension='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in {names[0]}:\n",
    "    number_points = decompositions[name]['number_of_splittings_after']\n",
    "    stat_plot_jumps(name,parameters,decompositions,number_points,saveas='Statistics '+name, plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second statistics plot will be the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_jumps_and_oscillation(name,parameters,decompositions,number_points,saveas='Statistics',plotting=True):\n",
    "    '''\n",
    "    This function plots, at each peak, the ratio between amount of oscillation in the measurement\n",
    "    to the right of the peak and the jump between the present peak to the previous one.\n",
    "    That is, at the j-th peak located at peak_j, we calculate\n",
    "    \n",
    "    ratio: oscillation[peak_j:end]/(abs( jump[j] - jump[j-1]))\n",
    "    \n",
    "    The ratio is distributed in three classes:\n",
    "    \n",
    "    ratio <= 1 (green color); 1<ratio <=2 (yellow color), and 2<ratio (red color)\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - number_points: number of peaks that we want to do statistics on\n",
    "    - saveas:  name  of saved figure\n",
    "    - plotting: plot figure (True) or not  (False)\n",
    "    ----------------\n",
    "    '''\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize=18)\n",
    "        plt.rc('ytick', labelsize=18)\n",
    "    \n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "    print(\"Statistics for Jumps and Oscillation, for \", name,\"\\n\")\n",
    "    parameters_now = parameters[name]\n",
    "    stop = parameters_now['stop']\n",
    "    E_now = parameters_now['energy']\n",
    "    xanes_now = parameters_now['xanes']\n",
    "\n",
    "    # ...decomposition properties\n",
    "    decomposition_now = decompositions[name]\n",
    "    jumps = decomposition_now['jumps']\n",
    "    peak_loc = decomposition_now['peak_location']\n",
    "    peak_height = decomposition_now['peak_heights']\n",
    "    omega_oscillation = decomposition_now['oscillation']\n",
    "    vector_dist_peak = decomposition_now['distance_between_peaks']\n",
    "    height_threshold = decomposition_now['height_threshold_evolution']\n",
    "    number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "    number_points = min(decomposition_now['number_of_splittings_after'], number_points)\n",
    "    \n",
    "    # New quantities\n",
    "    alpha_before = np.zeros(number_splittings_before)  # We shall put tehe alpha values here\n",
    "    jumps_before =jumps[0:number_splittings_before]\n",
    "    alpha_after = np.zeros(number_points-1)    # We shall put tehe alpha values here\n",
    "    jumps_after = jumps[number_splittings_before-1:]\n",
    "   \n",
    "    # Small plot of xanes measurement on top\n",
    "    where_to_stop_plot = int(.8*peak_loc[str(number_points-1)]+.2*stop)\n",
    "    \n",
    "    f,(ax1,ax2)= plt.subplots(2,1,sharex=True, gridspec_kw={'height_ratios': [1, 5]},figsize=(15,7))\n",
    "    ax1.set_yticks([0, .5, 1])\n",
    "    ax1.plot(E_now[0,:where_to_stop_plot].T,xanes_now[0,:where_to_stop_plot].T,color='blue',lw=4) # Plot xanes spectrum \n",
    "    ax1.grid(True)\n",
    "    \n",
    "    #To the right of the main peak\n",
    "    vector_proportion= []\n",
    "    \"We shall plot the xanes measurement and the peak variations\"\n",
    "    start_from =0\n",
    "    \n",
    "    for i in range(start_from,number_points-1):\n",
    "         ### Now we plot the h_threshold\n",
    "        _,_,h_threshold =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "    ############################\n",
    "        ### How much of the jump is noise?\n",
    "        proportion_noise_jump=h_threshold/jumps_after[i]\n",
    "        vector_proportion.append(proportion_noise_jump)\n",
    "        colors = ['g','y','r']\n",
    "        # Plot a marker\n",
    "        ax1.plot(E_now[0,peak_loc[str(i)]],xanes_now[0,peak_loc[str(i)]],\\\n",
    "                 color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='o',label=None,  markersize=8,alpha=1,lw=4)\n",
    "        \n",
    "        ax2.plot(E_now[0,peak_loc[str(i)]],proportion_noise_jump,\\\n",
    "                 color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='o',label=None,  markersize=12,alpha=1)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        #########################################\n",
    "    plt.title(\"Comparison: proportion of oscillation and jump\",size=28)\n",
    "    M=max(vector_proportion)\n",
    "    ax2.axhspan(-1, 1, facecolor='g', alpha=0.05)\n",
    "    ax2.axhspan( 1,2, facecolor='y', alpha=0.05)\n",
    "    ax2.axhspan( 2,3*M, facecolor='r', alpha=0.05)\n",
    "    plt.margins(0) \n",
    "    plt.xlabel('Breakpoint energy position (eV)',size=22)\n",
    "    plt.ylabel('log(oscillation/jump)',size=22)\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    figure_save(saveas,figure_extension='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = names[0]\n",
    "number_points = decompositions[name]['number_of_splittings_after']\n",
    "stat_jumps_and_oscillation(name,parameters,decompositions,number_points, saveas='Proportion_oscillation'+name,plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third statistics will come from distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_dist_zero(name,parameters,decompositions,number_points,saveas='Statistics',plotting=True):\n",
    "\n",
    "    '''\n",
    "    Comparison between different methods to compute distance threshold\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - plot_type: 'normalized' or 'raw'\n",
    "    - figure_extension: 'png' or 'eps'\n",
    "    - loc: number from {1,2, 3, 4,} where legend will be placed\n",
    "    - title: title in the plot\n",
    "    ----------------\n",
    "    '''    \n",
    "    ##########################\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize=18)\n",
    "        plt.rc('ytick', labelsize=18)\n",
    "    \n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "    print(\"Statistics for distance, for \", name,\"\\n\")\n",
    "    parameters_now = parameters[name]\n",
    "    stop = parameters_now['stop']\n",
    "    E_now = parameters_now['energy']\n",
    "    xanes_now = parameters_now['xanes']\n",
    "\n",
    "    # ...decomposition properties\n",
    "    decomposition_now = decompositions[name]\n",
    "    jumps = decomposition_now['jumps']\n",
    "    peak_loc = decomposition_now['peak_location']\n",
    "    peak_height = decomposition_now['peak_heights']\n",
    "    omega_oscillation = decomposition_now['oscillation']\n",
    "    vector_dist_peak = decomposition_now['distance_between_peaks']\n",
    "    height_threshold = decomposition_now['height_threshold_evolution']\n",
    "    number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "    number_points = min(decomposition_now['number_of_splittings_after'], number_points)\n",
    "    \n",
    "    '''\n",
    "    We shall plot the xanes measurement and the peak variations'''\n",
    "    #This value has to vary because the algorithm is assymetric\n",
    "    if number_splittings_before==0:\n",
    "        start_from =0 \n",
    "    else:\n",
    "        start_from = number_splittings_before-1\n",
    "       \n",
    "    vector_height_threshold_used= [height_threshold[str(j)] for j in range(number_points)]\n",
    "    vector_dist_peak_after=vector_dist_peak[start_from:]\n",
    "    vec_peak_loc = [peak_loc[str(j)] for j in range(number_points)]\n",
    "    jumps_after = jumps[start_from:]\n",
    "    \n",
    "    # Small plot of xanes measurement on top\n",
    "    where_to_stop_plot = int(.8*peak_loc[str(number_points-1)]+.2*stop)\n",
    "    \n",
    "    f,(ax1,ax2)= plt.subplots(2,1,sharex=True, gridspec_kw={'height_ratios': [1, 5]},figsize=(15,7))\n",
    "    ax1.set_yticks([0, .5, 1])\n",
    "    ax1.plot(E_now[0,:where_to_stop_plot].T,xanes_now[0,:where_to_stop_plot].T,color='blue',lw=4) # Plot xanes spectrum \n",
    "    ax1.grid(True)\n",
    "    \n",
    "    \n",
    "    for i in range(start_from,number_points-1):\n",
    "         ### Now we plot the h_threshold\n",
    "        _,_,h_threshold =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "    ############################\n",
    "        ### How much of the jump is noise?\n",
    "        proportion_noise_jump=h_threshold/jumps_after[i]\n",
    "        colors = ['g','y','r']\n",
    "        # Plot a marker\n",
    "        ax1.plot(E_now[0,peak_loc[str(i)]],xanes_now[0,peak_loc[str(i)]],\\\n",
    "                 color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='H',label=None,  markersize=16,alpha=.8,lw=4)   \n",
    "    \n",
    "    ## And now we are ready for the rest. We shall run the different statistics accross the model,\n",
    "    #and see how it goes\n",
    "    \n",
    "    first_dist = vector_dist_peak[start_from]\n",
    "    dist_learn_to_trust, dist_min_max, dist_regression = [first_dist],[first_dist],[first_dist]\n",
    "    \n",
    "    for i in range(1,number_points):\n",
    "        \n",
    "        ### We first compute the h_threshold\n",
    "        _,_,h_threshold =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "        ############################\n",
    "        ## Now we plot using min_max\n",
    "        hyperparameters['decay_rate_type']= 'min_max'\n",
    "        \n",
    "        _ ,dist_m_m ,_ , _ =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        \n",
    "        dist_min_max.append(dist_m_m)\n",
    "        \n",
    "        ############################\n",
    "        ## Now we plot using learn_to_trust\n",
    "        hyperparameters['decay_rate_type']= 'learn_to_trust'\n",
    "        \n",
    "        _ , dist_l_t,_ , _ =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        dist_learn_to_trust.append(dist_l_t)\n",
    "        \n",
    "        ############################\n",
    "        ## Now we plot using regression\n",
    "        hyperparameters['decay_rate_type']= 'regression'\n",
    "        \n",
    "        _ , dist_reg ,_ , _  =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        \n",
    "        dist_regression.append(dist_reg)\n",
    "        \n",
    "    #### NOW WE PLOT THEM ALL\n",
    "    # True distance\n",
    "    ax2.bar(E_now[0,vec_peak_loc][1:],vector_dist_peak_after[:number_points][1:],width=4,color='gray')\n",
    "    \n",
    "    gray_patch = mpatches.Patch(color='gray', label='True distance')\n",
    " \n",
    "    #min_max_plot\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],dist_min_max[1:],\\\n",
    "                 color='blue',label=None,marker='h',markersize=16,alpha=.4)\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Min_max')        \n",
    "    \n",
    "    #learn_to_trust plot\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],dist_learn_to_trust[1:],\\\n",
    "             color='orange',label=None,marker='o',markersize=16,alpha=.6)\n",
    "    orange_patch = mpatches.Patch(color='orange',alpha=.4, label='Learn_to_trust')\n",
    "    \n",
    "    #'estimated jump using least squares'\n",
    "    ## mew stands for \"marker edge width\"\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],dist_regression[1:],\\\n",
    "             color='red',label=None,marker='x',mew=4,markersize=16,alpha=.4)\n",
    "    red_patch = mpatches.Patch(color='red', label='Regression')\n",
    "    \n",
    "    ax2.grid(True)\n",
    "    plt.yscale('log')\n",
    "   #########################################\n",
    "    ax2.legend(handles=[gray_patch,blue_patch,orange_patch,red_patch],prop={'size':22})\n",
    "    plt.title(\"Study of decay rates for distance: \"+name,size=28)\n",
    "    plt.xlabel('Breakpoint energy position (eV)',size=22)\n",
    "    plt.ylabel('Log of estimated distance',size=22)\n",
    "    \n",
    "    #ax1.set_rasterized(True)\n",
    "    figure_save(saveas,figure_extension='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = names[0]\n",
    "decomposition_now= decompositions[name]\n",
    "\n",
    "number_points = decomposition_now['number_of_splittings_after']\n",
    "stat_dist_zero(name,parameters,decompositions,number_points,saveas='Statistics_distance',plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fianlly, we study the threshold for oscillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_thresholds(name,parameters,decompositions,number_points,saveas='Statistics',plotting=True):\n",
    "\n",
    "    '''\n",
    "    Comparison between different methods to compute height_threshold\n",
    "     ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - number_points: number of peaks that we want to do statistics on\n",
    "    - saveas:  name  of saved figure\n",
    "    - plotting: plot figure (True) or not  (False)\n",
    "   ----------------\n",
    "    '''\n",
    "    ##########################\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize=18)\n",
    "        plt.rc('ytick', labelsize=18)\n",
    "    \n",
    "    ##############################################################\n",
    "    # UNPACKING...\n",
    "    # ... parameters\n",
    "    print(\"Statistics for distance, for \", name,\"\\n\")\n",
    "    parameters_now = parameters[name]\n",
    "    stop = parameters_now['stop']\n",
    "    E_now = parameters_now['energy']\n",
    "    xanes_now = parameters_now['xanes']\n",
    "\n",
    "    # ...decomposition properties\n",
    "    decomposition_now = decompositions[name]\n",
    "    jumps = decomposition_now['jumps']\n",
    "    peak_loc = decomposition_now['peak_location']\n",
    "    peak_height = decomposition_now['peak_heights']\n",
    "    omega_oscillation = decomposition_now['oscillation']\n",
    "    vector_dist_peak = decomposition_now['distance_between_peaks']\n",
    "    height_threshold = decomposition_now['height_threshold_evolution']\n",
    "    number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "    number_points = min(decomposition_now['number_of_splittings_after'], number_points)\n",
    "    \n",
    "    '''\n",
    "    We shall plot the xanes measurement and the peak variations'''\n",
    "    #This value has to vary because the algorithm is assymetric\n",
    "    if number_splittings_before==0:\n",
    "        start_from =0 \n",
    "    else:\n",
    "        start_from = number_splittings_before-1\n",
    "    \n",
    "    vector_height_threshold_used= [height_threshold[str(j)] for j in range(number_points)]\n",
    "    vector_dist_peak_after=vector_dist_peak[start_from:]\n",
    "    vec_peak_loc = [peak_loc[str(j)] for j in range(number_points)]\n",
    "    jumps_after = jumps[start_from:]\n",
    "    \n",
    "    # Small plot of xanes measurement on top\n",
    "    where_to_stop_plot = int(.8*peak_loc[str(number_points-1)]+.2*stop)\n",
    "    \n",
    "    f,(ax1,ax2)= plt.subplots(2,1,sharex=True, gridspec_kw={'height_ratios': [1, 5]},figsize=(15,7))\n",
    "    ax1.set_yticks([0, .5, 1])\n",
    "    ax1.plot(E_now[0,:where_to_stop_plot].T,xanes_now[0,:where_to_stop_plot].T,color='blue',lw=4) # Plot xanes spectrum \n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ## And now we are ready for the rest. We shall run the different statistics accross the model,\n",
    "    #and see how it goes\n",
    "    \n",
    "    first_threshold = vector_height_threshold_used[start_from]\n",
    "    h_thresh_learn_to_trust, h_thresh_min_max, h_thresh_regression =[first_threshold],[first_threshold],[first_threshold]\n",
    "    \n",
    "    \n",
    "    _,_,h_thresh_t =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(0)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(0)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "    \n",
    "    h_thresh_true = [h_thresh_t]\n",
    "    \n",
    "    for i in range(1,number_points-1):\n",
    "        \n",
    "        ############################\n",
    "        ### Now we plot the h_threshold\n",
    "        _,_,h_thresh_t =\\\n",
    "        oscillation_function(\\\n",
    "                             np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                             full_computation=False)\n",
    "        \n",
    "        h_thresh_true.append(h_thresh_t)\n",
    "        ### How much of the jump is noise?\n",
    "        proportion_noise_jump=h_thresh_t/jumps_after[i]\n",
    "        colors = ['g','y','r']\n",
    "        # Plot a marker\n",
    "        ax1.plot(E_now[0,peak_loc[str(i)]],xanes_now[0,peak_loc[str(i)]],\\\n",
    "                 color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='H',label=None,  markersize=16,alpha=.8,lw=4)\n",
    "        \n",
    "        ############################\n",
    "        ## And now we are ready for the rest. We shall run the different statistics accross the model,\n",
    "        #and see how it goes\n",
    "        \n",
    "        ## Now we plot using min_max\n",
    "        \n",
    "        hyperparameters['decay_rate_type']= 'min_max'\n",
    "        \n",
    "        _ ,_ ,h_thres_m_m, _ =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        h_thresh_min_max.append(h_thres_m_m)\n",
    "        \n",
    "        ############################\n",
    "        ## Now we plot using learn_to_trust\n",
    "        hyperparameters['decay_rate_type']= 'learn_to_trust'\n",
    "        \n",
    "        _ ,_, h_thres_l_t, _ =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        \n",
    "        h_thresh_learn_to_trust.append(h_thres_l_t)\n",
    "        \n",
    "        ############################\n",
    "        ## Now we plot using regression\n",
    "        hyperparameters['decay_rate_type']= 'regression'\n",
    "        \n",
    "        _ ,_ ,h_thres_reg , _  =decay_rate(parameters,hyperparameters,name,\\\n",
    "            omega_oscillation, jumps,vector_height_threshold_used,\\\n",
    "            vector_peak_loc, vector_dist_peak_after[:i],i)\n",
    "        \n",
    "        h_thresh_regression.append(h_thres_reg)\n",
    "        \n",
    "    #### NOW WE PLOT THEM ALL\n",
    "    # True threshold\n",
    "    ax2.bar(E_now[0,vec_peak_loc][1:],h_thresh_true,width=4,color='gray')\n",
    "    gray_patch = mpatches.Patch(color='gray', label='True distance')\n",
    "    \n",
    "    #learn_to_trust plot\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],h_thresh_learn_to_trust,\\\n",
    "             color='orange',label=None,marker='H',markersize=16,alpha=.6)\n",
    "    orange_patch = mpatches.Patch(color='orange',alpha=.4, label='Learn_to_trust')\n",
    "    \n",
    "    #min_max_plot\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],h_thresh_min_max,\\\n",
    "                 color='blue',label=None,marker='x',mew=4,markersize=16,alpha=.4)\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Min_max')        \n",
    "    \n",
    "    #'estimated jump using least squares. NOTE: mew stands for \"marker edge width\"\n",
    "    ax2.plot(E_now[0,vec_peak_loc][1:],h_thresh_regression,\\\n",
    "             color='red',label=None,marker='o',mew=4,markersize=16,alpha=.4)\n",
    "    red_patch = mpatches.Patch(color='red', label='Regression')\n",
    "    \n",
    "    ax2.grid(True)\n",
    "    plt.yscale('log')\n",
    "   #########################################\n",
    "    \n",
    "    ax2.legend(handles=[gray_patch,blue_patch,orange_patch,red_patch],prop={'size':22})\n",
    "    plt.title(\"Evolution of height threshold and estimates\",size=28)\n",
    "    #ax1.set_rasterized(True)\n",
    "    plt.xlabel('Breakpoint energy position (eV)',size=22)\n",
    "    plt.ylabel('Log of estimated height threshold',size=22)\n",
    "    \n",
    "    figure_save(saveas,figure_extension='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = names[0]\n",
    "'''\n",
    "print(decompositions[name]['distance_between_peaks'])\n",
    "print(decompositions[name]['jumps'])\n",
    "print(decompositions[name]['peak_location'])                           \n",
    "   '''                        \n",
    "number_points = 15\n",
    "stat_thresholds(name,parameters,decompositions,number_points,saveas='Statistics_height_threshold',plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last statistics plot will be the following: \n",
    "    \n",
    "    1. We plot the jumps, and the threshold together\n",
    "    2. We shall also plot how much decay we are actually predicting using different methods (min_max, regression)\n",
    "    3. We also plot how much we are actually using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing number of points\n",
    "\n",
    "It is the case that the number of breakpoints in each XANES measurement varies. Hence, we need to adjust for that. The first code is as follows: it receives a bunch of intervals and points inside of it. Then it starts breaking it into smaller intervals by breaking the largest interval in two pieces.\n",
    "\n",
    "So, you have an interval $\\mathscr{I}$, with points $e_0 < e_1 <.... <e_M$. Now, given a number N >M we introduce new elements to this sequence until it has N intervals (that is, N+1 points).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_breakpoints(parameters,decompositions,names):\n",
    "    '''\n",
    "    This function receives towo dictionaries and one list(names), whose  elements are the keys to the\n",
    "    previously mentioned dictionaries. \n",
    "    \n",
    "    The function takes the breakpoints (the peaks) in each decomposition, at this point given as a dictionary,\n",
    "    and creats a vector from it.\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - names: names of materials studied\n",
    "    \n",
    "    ----------------\n",
    "    \n",
    "    '''\n",
    "    breakpoints ={}\n",
    "    for name in names:\n",
    "        \n",
    "        ##############################################################\n",
    "        # UNPACKING...\n",
    "        # ... parameters\n",
    "        xanes_now = parameters[name]['xanes']\n",
    "        E_now =parameters[name]['energy']\n",
    "        xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "    \n",
    "        # ...decomposition properties\n",
    "        decomposition_now = decompositions[name]\n",
    "        peak_loc = decomposition_now['peak_location']\n",
    "        number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "        number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "        dist_left = decomposition_now['distance_between_peaks'][:number_splittings_before]\n",
    "        dist_right = decomposition_now['distance_between_peaks'][number_splittings_before:]\n",
    "        breakpoints[name] = np.array([peak_loc[str(j)] for j in range(-number_splittings_before,number_splittings_after)],np.int32)\n",
    "  \n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the output will look more or less like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoints = clean_up_breakpoints(parameters,decompositions,names)\n",
    "\n",
    "breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_interval(breakpts,Normalization_number,kind='middle_domain'): #range_xanes,\n",
    "    \n",
    "    '''\n",
    "    Given breakpoints brekpts (integers), finds a superset of breakpoints that are in between\n",
    "    \n",
    "    min(breakpts) and max(breakpts)\n",
    "    \n",
    "    with Normalization_number of points. The criteria for the construction of this superset is \n",
    "    finds the largest subinterval in the domain and split it in two equally sized intervals\n",
    "    ----------------\n",
    "    \n",
    "    Input:\n",
    "    - breakpts: numpy array with breakpoints\n",
    "    - Normalization_number: number of breakpoints in the final set that containts the breakpoints brkpts\n",
    "                            initially given \n",
    "    - kind: type of interpolation. In principle one can use \n",
    "            - middle_domain: finds the largest subinterval in the domain and split it in two\n",
    "            - middle_image: finds the largest subinterval in the domain, say, interval J)\n",
    "                            and split it into the part that the range of its image through XANES, that is, \n",
    "                            XANES(J), varies up to half of its range  \n",
    "    ----------------\n",
    "    '''\n",
    "    \n",
    "    M = len(breakpts)\n",
    "    interval_size = np.array([breakpts[i] - breakpts[i-1] for i in range(1,M)],dtype=np.int16)\n",
    "\n",
    "    \n",
    "    if kind == 'middle_domain':\n",
    "        \n",
    "        while M<Normalization_number:\n",
    "            \n",
    "            Max_int = np.max(interval_size)\n",
    "            a = np.squeeze(np.min(np.where(interval_size == Max_int)))\n",
    "            \n",
    "            breakpts= np.concatenate((breakpts[:a+1],np.floor(np.array([breakpts[a]+Max_int/2])),breakpts[a+1:]))\n",
    "            M += 1\n",
    "            interval_size = np.concatenate((interval_size[:a],np.array([np.floor(Max_int/2),np.ceil(Max_int/2)]),\\\n",
    "                                        interval_size[a+1:]))\n",
    "        return  breakpts\n",
    "    \n",
    "    if kind =='middle_image':\n",
    "        while M<Normalization_number:\n",
    "            Max_int = np.max(interval_size)\n",
    "            # Find largest interval\n",
    "            a = np.squeeze(np.min(np.where(interval_size == Max_int)))\n",
    "            \n",
    "            # Find middle point in the range\n",
    "            Mid = (range_xanes[0,breakpts[a]]+range_xanes[0,breakpts[a+1]])/2\n",
    "            \n",
    "            #Now find first entry that satisfies  range[i]<M <=range[i+1]\n",
    "            v = range_xanes[0,breakpts[a]:breakpts[a+1]]\n",
    "            mid_pt = np.max([np.min(np.where(v>=Mid)),np.min(np.where(v<Mid))])\n",
    "            breakpts= \\\n",
    "            np.concatenate((breakpts[:a+1],np.array([breakpts[a]+mid_pt-1],dtype=np.int16),breakpts[a+1:]))\n",
    "            interval_size = \\\n",
    "            np.concatenate((interval_size[:a],np.array([breakpts[a+1] -breakpts[a],breakpts[a+2]-breakpts[a+1]],dtype=np.int16),\\\n",
    "                                        interval_size[a+1:]))\n",
    "            M += 1\n",
    "            \n",
    "        return  breakpts    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we give a sequence of vectors, we find a normalization number and split all of them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Normalization_number(decompositions,names,how_many_before,how_many_after):\n",
    "    '''\n",
    "    In order to feed some Machine Learning models, the input should have a fixed size. This function gives the\n",
    "    minimum size that the input should have. \n",
    "    \n",
    "    It goes like this: given a family decompositions indexed by names, \n",
    "    in each of them selects \n",
    "        min(how_many_before breakpoints.maximal_number of splittints before) \n",
    "        and\n",
    "        min(how_many_after breakpoints.maximal_number of splittints after)\n",
    "    \n",
    "    breakpoints. Then,\n",
    "    return the largest cardinality (number of points) among them. \n",
    "    \n",
    "    This number is a normalization index M: given all the supersets that contain each of these breakpoint indices,\n",
    "    the ones with same cardinality should have at least M points.\n",
    "    \n",
    "     ----------------\n",
    "    Input:\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - how_many_before: number of peaks that should be interpolated before 0th peak\n",
    "    - how_many_after: number of peaks that should be interpolated after 0th peak\n",
    "    \n",
    "    ---------------\n",
    "    Output:\n",
    "    - M:smallest cardinality among all supersets of indices\n",
    "    '''\n",
    "    \n",
    "    M=0\n",
    "    for name in names:\n",
    "        \n",
    "        ##############################################################\n",
    "        # UNPACKING...\n",
    "        # ... decomposition\n",
    "        decomposition_now = decompositions[name]\n",
    "        number_splittings_before = min(how_many_before,decomposition_now['number_of_splittings_before'])\n",
    "        number_splittings_after = min(how_many_after,decomposition_now['number_of_splittings_after'])\n",
    "    \n",
    "        if M <number_splittings_after +number_splittings_before:\n",
    "            M = number_splittings_after +number_splittings_before\n",
    "    return M\n",
    "\n",
    "def remove_duplicates(aux_breakpt):\n",
    "    '''\n",
    "    Receives vector with breakpoints and clean it, only leaving non-duplicate elements.\n",
    "    ----------------\n",
    "    Input:\n",
    "    -aux_breakpt: increasing vector with breakpoints, no duplicates\n",
    "    ----------------\n",
    "    Output:\n",
    "    -breakpoint: a increasing vector with breakpoint indices\n",
    "    '''\n",
    "   \n",
    "   \n",
    "    L = len(aux_breakpt)\n",
    "    breakpoint = np.array([aux_breakpt[0]])\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        if breakpoint[-1]< aux_breakpt[i]: \n",
    "            breakpoint = np.append(breakpoint,aux_breakpt[i])\n",
    "    return breakpoint\n",
    "\n",
    "def normalize_points(\n",
    "    parameters,names, decompositions,k=3,\n",
    "    kind='middle_domain',printing=True,how_many_before=20,how_many_after =20 \n",
    "):\n",
    "    '''\n",
    "    Receives a family of decompositions each with breakpoints B_name indexed by name in {names}, does the following:\n",
    "        1.find the minimum size of the input space (Normalization_number);\n",
    "        2. enlarge this number by a factor k (k>=1);\n",
    "        3. append new breakpoints for each of the initial sets B_name in order to obtain new sets of breakpoints\n",
    "            tilde_B_name with size k*Normalization_number (=: Norm_number)\n",
    "            \n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - k=3: parameter that multiplies normalization number. \n",
    "    - kind: type of interpolation. In principle one can use \n",
    "            - middle_domain: finds the largest subinterval in the domain and split it in two\n",
    "            - middle_image: finds the largest subinterval in the domain, say, interval J)\n",
    "                            and split it into the part that the range of its image through XANES, that is, \n",
    "                            XANES(J), varies up to half of its range\n",
    "    - printing: print computations (True) or not  (False)\n",
    "    \n",
    "    - how_many_before: upper bound on number of points to be sought on the leftt of 0th breakpoint \n",
    "                        (automatically set as 20)\n",
    "    - how_many_after: upper bound on number of points to be sought on the right of 0th breakpoint \n",
    "                        (automatically set as 20)\n",
    "    ----------------\n",
    "    Output:\n",
    "    -breakpoint: a increasing vector with breakpoint indices with sizes k*Normalization_number\n",
    "    -Norm_number: k*Normalization number\n",
    "    '''\n",
    "   \n",
    "    \n",
    "    Norm_number = np.ceil(k*Calculate_Normalization_number(decompositions,names,how_many_before,how_many_after))\n",
    "    \n",
    "    if printing: print(\"Number of breakpoints is:\", Norm_number)\n",
    "    \n",
    "    breakpts=clean_up_breakpoints(parameters,decompositions,names)\n",
    "    # Unpacking ...\n",
    "    for name in names:\n",
    "        ##############################################################\n",
    "        # UNPACKING...\n",
    "        # ... parameters\n",
    "        decomposition_now = decompositions[name]\n",
    "        \n",
    "        stop = parameters[name]['stop']\n",
    "        E_now = parameters[name]['energy']\n",
    "        xanes_now = parameters[name]['xanes']\n",
    "\n",
    "        # ...decomposition properties\n",
    "        decomposition_now = decompositions[name]\n",
    "        peak_loc = decomposition_now['peak_location']\n",
    "        number_splittings_before = min(how_many_before,decomposition_now['number_of_splittings_before'])\n",
    "        number_splittings_after = min(how_many_after,decomposition_now['number_of_splittings_after'])\n",
    "\n",
    "        aux_breakpt= np.array([np.int(peak_loc[str(i)]) for i in range(-number_splittings_before,number_splittings_after)],dtype=np.int16)    \n",
    "        breakpts[name] = remove_duplicates(aux_breakpt)\n",
    "        breakpts[name] = split_interval(breakpts[name],Norm_number,kind=kind)#xanes_now, \n",
    "        breakpts[name] = np.array(breakpts[name],dtype=np.int16)\n",
    "    return breakpts, Norm_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,names,decompositions,k=1.1,kind='middle_domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can definitely see that the points get better distributed in this fashion. Just for curiosity, I'll add the functionality that gives the $\\ell^{\\infty}$ norm of these approximations.\n",
    "\n",
    "Let's interpolate all these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.interpolate import interp1d, splrep, splev\n",
    "\n",
    "\n",
    "def interpolate_same_size(parameters,breakpts,names,kind,filename,bdry='unpadded',bdry_add=2,printing=True):\n",
    "    '''\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - breakpts: ordered breakpoints\n",
    "    - kind: type of interpolation. Types are\n",
    "            -\"equally_spaced_splines\": iterpolate using Cubic Splines in equally spaced meshgrid\n",
    "            -\"equally_spaced_polygonal\": iterpolate using Polygonals in equally spaced meshgrid\n",
    "            - \"cubicspline\" : iterpolate using Cubic Splines using interpolation points breakpts\n",
    "            -\"linear\": iterpolate using Polygonals using interpolation points breakpts\n",
    "            - \"both\": iterpolate using Cubic Splines and Polygonals using interpolation points breakpts\n",
    "            \n",
    "    - printing: print computations (True) or not  (False)\n",
    "    - filename: name of saved figure \n",
    "    - bdry='unpadded': if doing Cubic Splines, pad it with bdry_add points. \n",
    "                        In case of no padding, takes value 'unpadded'\n",
    "    - bdry_add: standard value 2, sets the number of extra points to add to the right and left of XANES measurement \n",
    "                in order to avoid boundary issues in XANES\n",
    "\n",
    "    ---------------\n",
    "    '''\n",
    "    \n",
    "    approximations={}\n",
    "    l_infty_errors={}\n",
    "    l_1_errors={}\n",
    "    \n",
    "    for name in names:\n",
    "        if printing: \n",
    "            plt.figure(figsize=(15,10))\n",
    "            plt.rc('font', family='serif')\n",
    "            plt.rc('xtick', labelsize=18)\n",
    "            plt.rc('ytick', labelsize=18)\n",
    "\n",
    "        ##############################################################\n",
    "        # UNPACKING...\n",
    "        # ... parameters\n",
    "        xanes_now = parameters[name]['xanes']\n",
    "        E_now =parameters[name]['energy']\n",
    "        xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "        # ... decomposition\n",
    "        decomposition_now = decompositions[name]\n",
    "        peak_loc = decomposition_now['peak_location']\n",
    "        number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "        number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "        start = max(peak_loc[str(-number_splittings_before)],breakpts[name][0])\n",
    "        stop = min(peak_loc[str(number_splittings_after-1)],breakpts[name][-1])+1\n",
    "        \n",
    "        full_dimension = stop-start-1\n",
    "\n",
    "        ####################################\n",
    "        if kind ==\"equally_spaced_splines\":\n",
    "\n",
    "            N = len(breakpts[name])\n",
    "            \n",
    "            ticks = np.linspace(start,stop,N,endpoint = True, dtype=np.int16)\n",
    "            ticks= ticks.reshape(1,-1)\n",
    "\n",
    "            if bdry=='unpadded':\n",
    "                x= E_now[0,ticks[0,:]]\n",
    "                y = xanes_now[0,ticks[0,:]]\n",
    "                cs = interp1d(x, y,kind='cubic')\n",
    "            elif bdry=='padded':\n",
    "                distance_left = E_now[0,ticks[0,1]]-E_now[0,ticks[0,0]]\n",
    "                distance_right = E_now[0,ticks[0,-1]]-E_now[0,ticks[0,-2]]\n",
    "\n",
    "                x = np.pad(np.array(E_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'linear_ramp', end_values=(E_now[0,ticks[0,0]] - bdry_add*distance_left, E_now[0,ticks[0,-1]] + bdry_add*distance_right))\n",
    "                x = np.squeeze(x)\n",
    "\n",
    "                y = np.pad(np.array(xanes_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'edge')\n",
    "                y=np.squeeze(y)\n",
    "\n",
    "                cs = interp1d(x, y,kind='cubic')\n",
    "\n",
    "            l_infty = np.round(max(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)\n",
    "            approximations[name]= cs\n",
    "            l_infty_errors[name] =l_infty\n",
    "            \n",
    "            l_1 = np.round(np.sum(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)  \n",
    "            l_1_errors[name] =l_1\n",
    "\n",
    "            if printing:\n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,xanes_now[0,start:stop].T,'--',\\\n",
    "                    label =name,color='red',lw=4\n",
    "                )\n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,cs(E_now[0,start:stop]).T,linestyle=(0,(5,1)), \\\n",
    "                    label=\"CS \"+\" $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty),color='blue',lw=4\n",
    "                )\n",
    "                plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "                plt.xlabel(\"Energy (eV)\", size=28)\n",
    "                plt.title('Eq. spaced cub. splines interpolation of XANES spectrum, N: '+str(N)+', full dim. : ' +str(full_dimension), size=22)\n",
    "                plt.legend(loc=4,prop={'size':22})\n",
    "                plt.grid(True)\n",
    "                figure_save(\"XANES_spline\"+filename+kind)\n",
    "                plt.show()\n",
    "\n",
    "        ####################################\n",
    "        if kind ==\"equally_spaced_polygonal\":\n",
    "            \n",
    "            N = len(breakpts[name])\n",
    "            ticks = np.linspace(start,stop,N,endpoint = True, dtype=np.int16)\n",
    "            ticks= ticks.reshape(1,-1)\n",
    "            \n",
    "            if bdry=='unpadded':\n",
    "                x= E_now[0,ticks[0,:]]\n",
    "                y = xanes_now[0,ticks[0,:]]\n",
    "                cs = interp1d(x, y,kind='linear')\n",
    "            elif bdry=='padded':\n",
    "                distance_left = E_now[0,ticks[0,1]]-E_now[0,ticks[0,0]]\n",
    "                distance_right = E_now[0,ticks[0,-1]]-E_now[0,ticks[0,-2]]\n",
    "                \n",
    "                x = np.pad(np.array(E_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'linear_ramp', end_values=(E_now[0,ticks[0,0]] - bdry_add*distance_left, E_now[0,ticks[0,-1]] + bdry_add*distance_right))\n",
    "                x = np.squeeze(x)\n",
    "                \n",
    "                y = np.pad(np.array(xanes_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'edge')\n",
    "                y=np.squeeze(y)\n",
    "                \n",
    "                ## Two versions: version 1            \n",
    "                cs = interp1d(x, y,kind='linear')\n",
    "                \n",
    "            l_infty = np.round(max(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)\n",
    "            approximations[name]= cs\n",
    "            l_infty_errors[name] =l_infty\n",
    "\n",
    "            l_1 = np.round(np.sum(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)  \n",
    "            l_1_errors[name] =l_1\n",
    "\n",
    "            \n",
    "            if printing:\n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,xanes_now[0,start:stop].T,'--',label =name,color='red',lw=4\n",
    "                )\n",
    "                \n",
    "                ## Two versions: version 1            \n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,cs(E_now[0,start:stop]).T,linestyle=(0,(5,1)),\\\n",
    "                    label=\"CS \"+\" $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty),color='blue',lw=4\n",
    "                )\n",
    "                plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "                plt.title('Eq. spaced polygonal interpolation of XANES spectrum, N: '+str(N)+', full dim. : ' +str(full_dimension), size=22)\n",
    "                plt.xlabel(\"Energy (eV)\", size=28)\n",
    "                plt.legend(loc=4,prop={'size':22})\n",
    "                plt.grid(True)\n",
    "                figure_save(\"XANES_spline\"+filename+kind)\n",
    "                plt.show()\n",
    "\n",
    "        ####################################\n",
    "        if kind ==\"cubicspline\" or kind=='both':\n",
    "            \n",
    "            ticks = np.array(breakpts[name],dtype=np.int16)\n",
    "            ticks= ticks.reshape(1,-1)\n",
    "            N = len(breakpts[name])\n",
    "            if bdry=='unpadded':\n",
    "                x= E_now[0,ticks[0,:]]\n",
    "                y = xanes_now[0,ticks[0,:]]\n",
    "                cs = interp1d(x, y,kind='cubic')\n",
    "                \n",
    "                l_infty = np.round(max(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)\n",
    "                l_1 = np.round(np.sum(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)  \n",
    "            elif bdry=='padded':\n",
    "                distance_left = E_now[0,ticks[0,1]]-E_now[0,ticks[0,0]]\n",
    "                distance_right = E_now[0,ticks[0,-1]]-E_now[0,ticks[0,-2]]\n",
    "                \n",
    "                x = np.pad(np.array(E_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'linear_ramp', end_values=(E_now[0,ticks[0,0]] - bdry_add*distance_left, E_now[0,ticks[0,-1]] + bdry_add*distance_right))\n",
    "                x = np.squeeze(x)\n",
    "                y = np.pad(np.array(xanes_now[0,ticks[0,:]]), (bdry_add, bdry_add), 'edge')\n",
    "                y=np.squeeze(y)\n",
    "                \n",
    "                cs = splrep(x, y,k=3)\n",
    "                y_2 = splev(E_now[0,start:stop], cs)           \n",
    "                l_infty = np.round(max(np.abs(xanes_now[0,start:stop]-y_2)),3)\n",
    "                l_1 = np.round(np.sum(np.abs(xanes_now[0,start:stop]-y_2)),3)  \n",
    "                \n",
    "            approximations[name]= cs\n",
    "            l_infty_errors[name] =l_infty\n",
    "            \n",
    "            l_1_errors[name] =l_1\n",
    "\n",
    "            \n",
    "            if printing:\n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,xanes_now[0,start:stop].T,'--',label =name,color='red',lw=4\n",
    "                )\n",
    "                if bdry=='unpadded':\n",
    "                    plt.plot(\n",
    "                        E_now[0,start:stop].T,cs(E_now[0,start:stop]).T,linestyle=(0,(5,1)),\\\n",
    "                        label=\"CS \"+\" $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty),color='blue',lw=4\n",
    "                    )\n",
    "                elif bdry=='padded':\n",
    "                    plt.plot(\n",
    "                        E_now[0,start:stop].T,y_2.T,linestyle=(0,(5,1)),\\\n",
    "                        label=\"CS \"+\" $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty),color='blue',lw=4\n",
    "                    )\n",
    "\n",
    "                plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "                plt.xlabel(\"Energy (eV)\", size=28)\n",
    "                plt.legend(loc=4,prop={'size':22})\n",
    "                plt.grid(True)\n",
    "               \n",
    "            if kind != 'both':\n",
    "                if printing:\n",
    "                    plt.title('CS interpolation of XANES spectrum, N: '+str(N)+', full dim. : ' +str(full_dimension), size=22)\n",
    "                    figure_save(\"XANES_spline\"+filename+kind)\n",
    "                    plt.show()\n",
    "\n",
    "        ####################################\n",
    "        if kind ==\"linear\" or kind=='both': #    hence it is polynomial type\n",
    "            N = len(breakpts[name])\n",
    "            ticks = np.array(breakpts[name],dtype=np.int16)\n",
    "            ticks= ticks.reshape(1,-1)\n",
    "            x= E_now[0,ticks[0,:]]\n",
    "            y = xanes_now[0,ticks[0,:]]\n",
    "            cs = interp1d(x, y,kind='linear')\n",
    "            l_infty = np.round(max(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)\n",
    "            if printing:\n",
    "                plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "                plt.xlabel(\"Energy (eV)\", size=28)\n",
    "                if kind !='both':\n",
    "                    plt.plot(\n",
    "                        E_now[0,start:stop].T,xanes_now[0,start:stop].T,'--',label =name,color='red',lw=4\n",
    "                    )\n",
    "                plt.plot(\n",
    "                    E_now[0,start:stop].T,cs(E_now[0,start:stop]).T,linestyle=(0,(5,1)),\n",
    "                    label =\"Polygonal $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty),color='blue',lw=4\n",
    "                )\n",
    "                plt.legend(loc=4,prop={'size':22})\n",
    "                plt.grid(True)\n",
    "            approximations[name]= cs,\n",
    "            l_infty_errors[name] =l_infty\n",
    "             \n",
    "            l_1 = np.round(np.sum(np.abs(xanes_now[0,start:stop]-cs(E_now[0,start:stop]))),3)  \n",
    "            l_1_errors[name] =l_1\n",
    "    \n",
    "            if kind !='both': \n",
    "                if printing:\n",
    "                    plt.title('Polygonal interpolation of XANES spectrum , N: '+str(N)+', full dim. : ' +str(full_dimension), size=28)\n",
    "                    figure_save(\"XANES_polygonal\"+filename+kind)\n",
    "            else:    \n",
    "                if printing:\n",
    "                    plt.title('Polygonal and splines interpolation of XANES spectrum , N: '+str(N)+', full dim. : ' +str(full_dimension), size=28)\n",
    "                    figure_save(\"XANES_polygonal_and_splines_same_size\"+filename+kind)\n",
    "        if printing:\n",
    "            plt.show()\n",
    "\n",
    "    return approximations,l_infty_errors, l_1_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,names,decompositions,k=1.1,kind='middle_domain')\n",
    "\n",
    "_,_,_=interpolate_same_size(parameters,breakpts,names,'cubicspline',\"allxanes_unnormalized\",bdry='unpadded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's interpolate only before main peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we only want to interpolate on the pre-peak we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_splittings_after_zeros = dict.fromkeys(parameters, 1)\n",
    "number_splittings_after_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such acase, we also need to change the stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_at_main_peak = dict.fromkeys(stopping, 0)\n",
    "\n",
    "for name in names:\n",
    "    decomposition_now = decompositions[name]\n",
    "    peak_loc = decomposition_now['peak_location']\n",
    "    stop_at_main_peak[name] = peak_loc[str(0)]\n",
    "\n",
    "stop_at_main_peak   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts_only_before,_ = normalize_points(\n",
    "    parameters, names, decompositions,k=2,kind='middle_domain',how_many_after=1\n",
    ")\n",
    "\n",
    "breakpts_only_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts_only_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_same_size(\n",
    "    parameters,breakpts_only_before,{names[1]},'linear',\"allxanesbefore\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,{names[1]},decompositions,k=1.1,kind='middle_domain')\n",
    "interpolate_same_size(parameters,breakpts,{names[1]},'cubicspline',\"allxanesbefore\",bdry='unpadded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we test the case of equally spaced interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,{names[1]},decompositions,k=1.1,kind='middle_domain')\n",
    "interpolate_same_size(parameters,breakpts,{names[1]},'equally_spaced_splines',\"allxanes_unnormalized\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,{names[1]},decompositions,k=1.1,kind='middle_domain')\n",
    "interpolate_same_size(parameters,breakpts,{names[1]},'equally_spaced_polygonal',\"allxanes_unnormalized\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,{names[1]},decompositions,k=1.1,kind='middle_domain')\n",
    "interpolate_same_size(parameters,breakpts,{names[1]},'linear',\"allxanes_unnormalized\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to implement the approximation using clamped splines. The idea here is a bit different though: we shall interpolate inside each sub interval, finding then a cubic splines whose derivatives on the boundary are 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def clamped_interpolation(parameters,breakpts,decompositions,names,printing=True,derivative=[0]):\n",
    "    '''\n",
    "    ----------------\n",
    "    Input:\n",
    "    - parameters: dictionary with materials' properties\n",
    "    - names: names of materials studied\n",
    "    - decomposition: dictionary with information about peaks and their locations\n",
    "    - derivative: plot derivative of order [0], [1], or [0, 1]\n",
    "    - breakpts: ordered breakpoints\n",
    "    - printing: print computations (True) or not  (False)\n",
    "    ----------------\n",
    "    ''' \n",
    "    if printing:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize=28)\n",
    "        plt.rc('ytick', labelsize=28)\n",
    "        ax =plt.subplot(111)\n",
    "        \n",
    "    cubic_splines_clamped ={}\n",
    "    \n",
    "    for name in names:\n",
    "        \n",
    "        ##############################################################\n",
    "        # UNPACKING...\n",
    "        # ... parameters\n",
    "        cubic_splines_clamped[name] = {}\n",
    "        decomposition_now = decompositions[name]\n",
    "\n",
    "        xanes_now = parameters[name]['xanes']\n",
    "        E_now =parameters[name]['energy']\n",
    "        xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "        \n",
    "        peak_loc = decomposition_now['peak_location']\n",
    "\n",
    "        number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "        number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "        \n",
    "        l_infty, l_1 = 0, 0\n",
    "        \n",
    "        ticks = np.array(breakpts[name],dtype=np.int16)\n",
    "        #print(\"ticks\",ticks)\n",
    "        ticks= ticks.reshape(1,-1)\n",
    "        \n",
    "        ### There is an issue with peak_locs: it can have duplicates; we need to clean them \n",
    "        # uncleaned\n",
    "        vector_peak_loc = np.array([peak_loc[str(i)]  for i in range(-number_splittings_before,number_splittings_after)])\n",
    "        #...cleaning\n",
    "        vector_peak_loc = remove_duplicates(vector_peak_loc)\n",
    "        \n",
    "        ## Truncate to the range of the pbreakpoints:\n",
    "        vector_peak_loc = vector_peak_loc[np.where(vector_peak_loc >=breakpts[name][0])]\n",
    "        vector_peak_loc = vector_peak_loc[np.where(vector_peak_loc <=breakpts[name][-1])]\n",
    "        \n",
    "        start = vector_peak_loc[0]\n",
    "        stop = vector_peak_loc[-1]\n",
    "        \n",
    "        full_dimension = stop-start\n",
    "        if printing:\n",
    "            plt.plot(\n",
    "            E_now[0,start:stop+1].T,xanes_now[0,start:stop+1].T,'-',color='red',lw=4\n",
    "        )\n",
    "        \n",
    "        N = np.shape(ticks)[1]\n",
    "        for i in range(len(vector_peak_loc)-1):\n",
    "            left, right = vector_peak_loc[i] ,vector_peak_loc[i+1]\n",
    "            #print(i, left,right)\n",
    "            x= E_now[0,left:right+1]\n",
    "            y = xanes_now[0,left:right+1]\n",
    "            \n",
    "            range_of_interest = np.where((ticks>=left)*(ticks<=right) ==True)[1]\n",
    "            \n",
    "            selected_points = ticks[0,range_of_interest]\n",
    "            \n",
    "            x_interp = np.squeeze(E_now[0,selected_points])\n",
    "            y_interp = np.squeeze(xanes_now[0,selected_points])\n",
    "            \n",
    "            cs_glue = CubicSpline(x_interp, y_interp,bc_type='clamped')\n",
    "            \n",
    "            ## calculate error on each interval\n",
    "            v = np.array(y-cs_glue(np.array(x)),ndmin=1)\n",
    "            #  l_infty error\n",
    "            l_infty_in_this_interval= max(np.abs(v))\n",
    "            l_infty = max(float(l_infty),l_infty_in_this_interval)\n",
    "            \n",
    "            #  l_1 error\n",
    "            l_1 += np.sum(np.abs(v))\n",
    "            \n",
    "            cubic_splines_clamped[name].update(\n",
    "                {\n",
    "                str(i)+\"cubic_spline\":cs_glue,\n",
    "                str(i)+\"points_interp\":x_interp,\n",
    "                str(i)+\"x_interval\":x,\n",
    "                str(i)+\"l_infty_in_this_interval\":l_infty_in_this_interval\n",
    "                }\n",
    "            )\n",
    "        \n",
    "            if printing:\n",
    "                for d in derivative:\n",
    "                    colors = ['b','g']\n",
    "                    plt.plot(\n",
    "                        x.T,cs_glue(x,d).T,color=colors[d],lw=4,linestyle='--'\n",
    "                    )\n",
    "        cubic_splines_clamped[name].update(\n",
    "            {\n",
    "            \"l_infty\":l_infty,\n",
    "                \"l_1\":l_1,\n",
    "            \"vector_peak_loc\":vector_peak_loc    \n",
    "            }\n",
    "        )\n",
    "        \n",
    "    if printing:\n",
    "        \n",
    "        blue_patch = mpatches.Patch(\n",
    "            color='blue', label='GCCS.\\n $\\ell^{\\infty}$ error:'+str(np.round(l_infty,3))\n",
    "        )\n",
    "        red_patch = mpatches.Patch(color='red',label = 'Xanes: '+name)\n",
    "        \n",
    "        ax.legend(handles=[red_patch, blue_patch],loc=1,prop={'size':18})\n",
    "        plt.ylabel('Absorption $\\mu(E)$',size=28)\n",
    "        plt.xlabel(\"Energy (eV)\", size=28)\n",
    "        plt.title('GCCS of XANES spectrum, N: '+str(N)+', full dim. : ' +str(full_dimension), size=28)\n",
    "        plt.grid(True)\n",
    "        figure_save(\"clamped_splines_of\"+name+\"with\"+str(N)+\"points\")\n",
    "        plt.show()    \n",
    "    \n",
    "    return cubic_splines_clamped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = {names[1]}\n",
    "k_test = 1.1\n",
    "breakpts,N = normalize_points(parameters,name,decompositions,k=k_test,kind='middle_domain')\n",
    "\n",
    "\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,name,printing=True,derivative=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output \"clamped_output\" is a dictionary. Let's check its keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamped_outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know... it looks messy.  But the key that we shall use the most is the l_infty error key. For instance, in the case of FeO, the l_infty error is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamped_outcome[names[1]]['l_infty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = {names[3]}\n",
    "k_test = 1.1\n",
    "breakpts,N = normalize_points(parameters,name,decompositions,k=k_test,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,name,printing=True,derivative=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, with an usual spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'equally_spaced_splines'\n",
    "interpolate_same_size(\n",
    "    parameters,breakpts,{names[3]},type_interp,type_interp+\"of\"+names[3]+\"with\"+str(int(N))+\"points\",\\\n",
    "    bdry='unpadded',bdry_add=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpts,_ = normalize_points(parameters,{names[3]},decompositions,k=1.1,kind='middle_domain')\n",
    "type_interp ='cubicspline'\n",
    "interpolate_same_size(parameters,breakpts,{names[3]},type_interp,type_interp+\"of\"+names[3]+\"with\"+str(int(N))+\"points\",bdry='unpadded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the polygonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'equally_spaced_polygonal'\n",
    "\n",
    "interpolate_same_size(parameters,breakpts,{names[3]},type_interp,type_interp+\"of\"+names[3]+\"with\"+str(int(N))+\"points\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'linear'\n",
    "\n",
    "interpolate_same_size(parameters,breakpts,{names[3]},type_interp,type_interp+\"of\"+names[3]+\"with\"+str(int(N))+\"points\",bdry='padded',bdry_add=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Error decay as N increases: $\\ell^{\\infty}$ case</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S = 40\n",
    "errors = np.zeros([len(names),S])\n",
    "Norm_number = np.zeros([S])\n",
    "type_interp = 'linear'\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number[i] = normalize_points(parameters, {names[0]}, decompositions,k=1+0.1*i,printing=False)\n",
    "    _, l_infty_errors,_=interpolate_same_size(parameters,breakpts,{names[0]},type_interp,\"allxanes\",bdry='unpadded',bdry_add=4,printing=False)\n",
    "    for j in range(len({names[0]})):\n",
    "        errors[j,i] = l_infty_errors[names[j]]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len({names[0]})):\n",
    "    plt.plot(\n",
    "        Norm_number,errors[i,:],color='C'+str(i),linestyle='-',lw=4\n",
    "    )\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$\\ell^{\\infty}$ error for interpolation of type '+type_interp,size=28)\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "figure_save(\"l_infty_interp_error\"+type_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same for clamped splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 40\n",
    "errors = np.zeros([len(names),S])\n",
    "Norm_number = np.zeros([S])\n",
    "type_interp = 'clamped'\n",
    "\n",
    "name = names[1]\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number[i] = normalize_points(parameters, {name}, decompositions,k=1+0.1*i,printing=False)\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[0])\n",
    "    \n",
    "    for j in range(len({name})):\n",
    "        l_infty_errors[names[j+1]]=clamped_outcome[names[j+1]]['l_infty']\n",
    "        errors[j,i] = l_infty_errors[names[j+1]]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len({names[0]})):\n",
    "    plt.plot(\n",
    "        Norm_number,errors[i,:],color='C'+str(i),linestyle='-',lw=4\n",
    "    )\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$\\ell^{\\infty}$ error for interpolation of type '+type_interp,size=28)\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "figure_save(\"l_infty_interp_error\"+type_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S = 25\n",
    "errors_lin= np.zeros([len(names),S])\n",
    "errors_eq_lin = np.zeros([len(names),S])\n",
    "errors_eq_clamped = np.zeros([len(names),S])\n",
    "Norm_number_before = np.zeros([S])\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number_before[i] = normalize_points(parameters, names, decompositions,k=1+0.1*i,printing=False)\n",
    "    type_interp_1 = 'linear'\n",
    "    _, l_infty_errors,_=interpolate_same_size(parameters,breakpts,names,type_interp_1,\"allxanes\",bdry='padded',bdry_add=4,printing=False)\n",
    "\n",
    "    type_interp_2 = 'equally_spaced_polygonal'\n",
    "    _, l_infty_errors_eq_lin,_=interpolate_same_size(parameters,breakpts,names,type_interp_2,\"allxanes\",bdry='padded',bdry_add=4,printing=False)\n",
    "    \n",
    "    type_interp_3 = 'Clamped splines'\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,names,printing=False,derivative=[0])\n",
    "    for j in range(len(names)):\n",
    "        errors_lin[j,i] = l_infty_errors[names[j]]\n",
    "        errors_eq_lin[j,i] = l_infty_errors_eq_lin[names[j]]\n",
    "        errors_eq_clamped[j,i] = clamped_outcome[names[j]]['l_infty']\n",
    "\n",
    "plt.figure(figsize=(15,15))    \n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len(names)): \n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_lin[i,:],linestyle='-',color= 'C'+str(i),\\\n",
    "        label='Polygonal: '+names[i],lw=4,alpha=.4\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_lin[i,:],linestyle=(0, (5,1)),color= 'C'+str(i),\\\n",
    "        label='Polygonal, equally spaced: '+names[i],lw=4,alpha=.4\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_clamped[i,:],linestyle=(0, (3, 1, 1, 1, 1, 1)),color= 'C'+str(i),\\\n",
    "        label='GCCS: '+names[i],lw=4\n",
    "    )\n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$\\ell^{\\infty}$ error comparison:\\n Cub. Spline (clamped) versus polyg. interpolations',size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "plt.ylabel('log ($\\ell^{\\infty}$ error)',size=28)\n",
    "\n",
    "plt.yscale('log')\n",
    "figure_save(\"l_infty_interp_error_comparison_all_spline_types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S = 25\n",
    "errors_lin= np.zeros([len(names),S])\n",
    "errors_eq_lin = np.zeros([len(names),S])\n",
    "errors_eq_clamped = np.zeros([len(names),S])\n",
    "Norm_number_before = np.zeros([S])\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number_before[i] = normalize_points(parameters, names, decompositions,k=1+0.1*i,printing=False)\n",
    "    type_interp_1 = 'cubicspline'\n",
    "    _, l_infty_errors,_=interpolate_same_size(parameters,breakpts,names,type_interp_1,\"allxanes\",bdry='unpadded',bdry_add=0,printing=False)\n",
    "    \n",
    "    type_interp_2 = 'equally_spaced_splines'\n",
    "    _, l_infty_errors_eq_lin,_=interpolate_same_size(parameters,breakpts,names,type_interp_2,\"allxanes\",bdry='unpadded',bdry_add=0,printing=False)\n",
    "    \n",
    "    type_interp_3 = 'Clamped splines'\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,names,printing=False,derivative=[0])\n",
    "    for j in range(len(names)):\n",
    "        errors_lin[j,i] = l_infty_errors[names[j]]\n",
    "        errors_eq_lin[j,i] = l_infty_errors_eq_lin[names[j]]\n",
    "        errors_eq_clamped[j,i] = clamped_outcome[names[j]]['l_infty']\n",
    "\n",
    "plt.figure(figsize=(15,15))    \n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len(names)):    #color='C'+str(i)\n",
    "    '''\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_lin[i,:],linestyle='-',color= 'C'+str(i),label='CS: '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "    '''\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_lin[i,:],linestyle='-',color= 'C'+str(i),label='CS: equally spaced '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_clamped[i,:],linestyle=(0, (3, 1, 1, 1, 1, 1)),color= 'C'+str(i),\\\n",
    "        label='GCCS: '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "   \n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$\\ell^{\\infty}$ error comparison:\\n GCCS versus CS (unpadded)',size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "plt.ylabel('log ($\\ell^{\\infty}$ error)',size=28)\n",
    "figure_save(\"l_infty_interp_error\"+type_interp_1+type_interp_2+type_interp_3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Error decay as N increases: $l_{1}$ case</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 25\n",
    "errors_lin= np.zeros([len(names),S])\n",
    "errors_eq_lin = np.zeros([len(names),S])\n",
    "errors_eq_clamped = np.zeros([len(names),S])\n",
    "Norm_number_before = np.zeros([S])\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number_before[i] = normalize_points(parameters, names, decompositions,k=1+0.1*i,printing=False)\n",
    "    type_interp_1 = 'linear'\n",
    "    _,_, l_1_errors=interpolate_same_size(parameters,breakpts,names,type_interp_1,\"allxanes\",bdry='padded',bdry_add=4,printing=False)\n",
    "\n",
    "    type_interp_2 = 'equally_spaced_polygonal'\n",
    "    _,_, l_1_errors_eq_lin=interpolate_same_size(parameters,breakpts,names,type_interp_2,\"allxanes\",bdry='padded',bdry_add=4,printing=False)\n",
    "    \n",
    "    type_interp_3 = 'Clamped splines'\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,names,printing=False,derivative=[0])\n",
    "    for j in range(len(names)):\n",
    "        errors_lin[j,i] = l_1_errors[names[j]]\n",
    "        errors_eq_lin[j,i] = l_1_errors_eq_lin[names[j]]\n",
    "        errors_eq_clamped[j,i] = clamped_outcome[names[j]]['l_1']\n",
    "\n",
    "plt.figure(figsize=(15,15))    \n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len(names)): \n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_lin[i,:],linestyle='-',color= 'C'+str(i),\\\n",
    "        label='Polygonal: '+names[i],lw=4,alpha=.4\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_lin[i,:],linestyle=(0, (5,1)),color= 'C'+str(i),\\\n",
    "        label='Polygonal, equally spaced: '+names[i],lw=4,alpha=.4\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_clamped[i,:],linestyle=(0, (3, 1, 1, 1, 1, 1)),color= 'C'+str(i),\\\n",
    "        label='GCCS: '+names[i],lw=4\n",
    "    )\n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$l_{1}$ error comparison:\\n Cub. Spline (clamped) versus polyg. interpolations',size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "plt.ylabel('log ($l_{1}$ error)',size=28)\n",
    "\n",
    "plt.yscale('log')\n",
    "figure_save(\"l_1_interp_error_comparison_all_spline_types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 25\n",
    "errors_lin= np.zeros([len(names),S])\n",
    "errors_eq_lin = np.zeros([len(names),S])\n",
    "errors_eq_clamped = np.zeros([len(names),S])\n",
    "Norm_number_before = np.zeros([S])\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts, Norm_number_before[i] = normalize_points(parameters, names, decompositions,k=1+0.1*i,printing=False)\n",
    "    type_interp_1 = 'cubicspline'\n",
    "    _,_, l_1_errors=interpolate_same_size(parameters,breakpts,names,type_interp_1,\"allxanes\",bdry='unpadded',bdry_add=0,printing=False)\n",
    "    \n",
    "    type_interp_2 = 'equally_spaced_splines'\n",
    "    _,_, l_1_errors_eq_lin=interpolate_same_size(parameters,breakpts,names,type_interp_2,\"allxanes\",bdry='unpadded',bdry_add=0,printing=False)\n",
    "    \n",
    "    type_interp_3 = 'Clamped splines'\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,names,printing=False,derivative=[0])\n",
    "    for j in range(len(names)):\n",
    "        errors_lin[j,i] = l_1_errors[names[j]]\n",
    "        errors_eq_lin[j,i] = l_1_errors_eq_lin[names[j]]\n",
    "        errors_eq_clamped[j,i] = clamped_outcome[names[j]]['l_1']\n",
    "\n",
    "plt.figure(figsize=(15,15))    \n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "for i in range(len(names)):    #color='C'+str(i)\n",
    "    '''\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_lin[i,:],linestyle='-',color= 'C'+str(i),\\\n",
    "        label='CS: '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "    '''\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_lin[i,:],linestyle='-',color= 'C'+str(i),\\\n",
    "        label='CS: equally spaced '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "    plt.plot(\n",
    "        Norm_number_before,errors_eq_clamped[i,:],linestyle=(0, (3, 1, 1, 1, 1, 1)),color= 'C'+str(i),\\\n",
    "        label='GCCS: '+names[i],lw=4,alpha=1\n",
    "    )\n",
    "   \n",
    "    plt.grid(True)\n",
    "\n",
    "plt.title('$\\ell^{1}$ error comparison:\\n GCCS versus CS (unpadded)',size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of interpolation points',size=28)\n",
    "plt.ylabel('log ($\\ell^{1}$ error)',size=28)\n",
    "figure_save(\"l_1_interp_error\"+type_interp_1+type_interp_2+type_interp_3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between finite differences derivative and spline derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S = 40\n",
    "errors_lin= np.zeros([len(names),S])\n",
    "errors_eq_lin = np.zeros([len(names),S])\n",
    "errors_eq_clamped = np.zeros([len(names),S])\n",
    "Norm_number_before = np.zeros([S])\n",
    "name= names[0]\n",
    "\n",
    "## Unpacking...\n",
    "# ...parameters\n",
    "E_now = parameters[name]['energy']\n",
    "start=0\n",
    "stop = parameters[name]['stop']\n",
    "\n",
    "for i in range(S):\n",
    "    breakpts_only_before, Norm_number_before[i] = normalize_points(\n",
    "        parameters, {name}, decompositions,k=1+0.1*i,printing=False\n",
    "    )\n",
    "    type_interp_1 = 'cubicspline'\n",
    "    approx, l_infty_errors_lin,_=\\\n",
    "    interpolate_same_size(\n",
    "        parameters,breakpts,{name},type_interp_1,\"allxanes\",bdry='padded',bdry_add=4,printing=False\n",
    "    )\n",
    "    cs = approx[name]\n",
    "    ##compute the firs derivative\n",
    "    y = splev(E_now[0,start:stop], cs, der=1)           \n",
    "    \n",
    "    ## Now the clamped spline\n",
    "    breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "    \n",
    "    for name in {name}:\n",
    "        Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "        \n",
    "        clamped_now = clamped_outcome[name]\n",
    "        for i in range(Number_intervals):\n",
    "                # Unpacking....\n",
    "                ### ....clamped_now\n",
    "                cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "                x = clamped_now[str(i)+\"x_interval\"]\n",
    "                d=1\n",
    "                plt.plot(\n",
    "                    x.T,cs_glue(x,d).T,color='C'+str(d),lw=4,linestyle='--'\n",
    "                )\n",
    "    \n",
    "    x=E[name][0,0:stop]\n",
    "    y=materials[name][0,0:stop]\n",
    "   \n",
    "    #cs = approx[name]\n",
    "    plt.plot(\n",
    "        x,y,'--',label =\"Cubic splines $\\Vert\\cdot \\Vert_{\\infty}}$:\"+str(l_infty_errors_lin[name])+\\\n",
    "        \", Number of points:\"+str(N),color='C'+str(2*i%10),alpha=.2+.5*i,lw=4\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "## Unpacking...\n",
    "# ...parameters\n",
    "E_now = parameters[name]['energy']\n",
    "start=0\n",
    "xanes_now =parameters[name]['xanes']\n",
    "stop = stop_at_main_peak[name]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "\n",
    "##############################################################\n",
    "for i in range(1,L):\n",
    "    finite_difference_first[i]= (xanes_now[0,i+1]-xanes_now[0,i-1])/(E_now[0,i+1]-E_now[0,i-1])\n",
    "\n",
    "for i in {2}:\n",
    "    \n",
    "    breakpts,N_pts = normalize_points(\n",
    "        parameters,{name}, decompositions,k=1.9)\n",
    "    type_interp_1 = 'cubicspline'\n",
    "    approx, l_infty_errors_lin,_=\\\n",
    "    interpolate_same_size(\n",
    "        parameters,breakpts,{name},type_interp_1,\"allxanes\",bdry='padded',bdry_add=4,printing=False\n",
    "    )\n",
    "    cs = approx[name]\n",
    "    ##compute the firs derivative\n",
    "    plt.plot(\n",
    "        E_now[0,1:stop+1],splev(E_now[0,1:stop+1],cs,der=1),'--',label =\"Cubic splines $\\Vert\\cdot \\Vert_{\\infty}}$:\",\\\n",
    "        color='red',lw=4\n",
    "    )\n",
    "    plt.plot(\n",
    "        E_now[0,1:stop],finite_difference_first[1:stop],'--',label =\"Finite difference first order\",\\\n",
    "        color='#1f77b4',lw=4\n",
    "    )\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        plt.plot(\n",
    "            x.T,cs_glue(x,d).T,color='orange',lw=4,linestyle='--'\n",
    "        )\n",
    "\n",
    "        \n",
    "plt.grid(True)\n",
    "plt.legend(loc=2,prop={'size':22})\n",
    "figure_save(\"first_Derivative\"+name+type_interp)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare with the derivatives obtained through athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "xanes_now = parameters[name]['xanes']\n",
    "E_now =parameters[name]['energy']\n",
    "xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "\n",
    "# ... decomposition\n",
    "decomposition_now = decompositions[name]\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "            \n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "        \n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "\n",
    "xanes_now =np.array(xanes_now[0,start:stop],ndmin=2)\n",
    "domain_now = np.array(E_now[0,start:stop],ndmin=2)\n",
    "for i in range(2,L-1):\n",
    "    finite_difference_first[i]= (xanes_now[0,i+1]-xanes_now[0,i])/(E_now[0,i+1]-E_now[0,i])\n",
    "\n",
    "for i in {2}:\n",
    "    \n",
    "    breakpts,N_pts = normalize_points(\n",
    "        parameters,{name}, decompositions,k=1.9)\n",
    "    type_interp_1 = 'cubicspline'\n",
    "    approx, l_infty_errors_lin,_=\\\n",
    "    interpolate_same_size(\n",
    "        parameters,breakpts,{name},type_interp_1,\"allxanes\",bdry='padded',bdry_add=4,printing=False\n",
    "    )\n",
    "    cs = approx[name]\n",
    "    ##compute the firs derivative\n",
    "    plt.plot(\n",
    "        E_now[0,1:stop+1],splev(E_now[0,1:stop+1],cs,der=1),'--',label =\"Cubic splines $\\Vert\\cdot \\Vert_{\\infty}}$:\",\\\n",
    "        color='red',lw=4\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        E_now[0,1:stop],finite_difference_first[1:stop],'--',label =\"Finite difference first order\",\\\n",
    "        color='#1f77b4',lw=4\n",
    "    )\n",
    "    \n",
    "    plt.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,.3*raw_materials_1st_derivatives[names_1st_derivatives[0]][0,start:stop].T,color='green',label=\"Athena derivative\"\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    \n",
    "    plt.title(\"First Derivative of XANES spectrum of \"+str(name)+\"with\"+str(N_pts)+\"of interpolation\",fontsize=28)\n",
    "    plt.grid(True)\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        plt.plot(\n",
    "            x.T,cs_glue(x,d).T,color='orange',lw=4,linestyle='--'\n",
    "        )\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=22)\n",
    "figure_save(\"first_Derivative_comparison\"+name+type_interp)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the Athena and the clamped spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "xanes_now = parameters[name]['xanes']\n",
    "E_now =parameters[name]['energy']\n",
    "xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "\n",
    "# ... decomposition\n",
    "decomposition_now = decompositions[name]\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "            \n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "        \n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "\n",
    "xanes_now =np.array(xanes_now[0,start:stop],ndmin=2)\n",
    "domain_now = np.array(E_now[0,start:stop],ndmin=2)\n",
    "\n",
    "f,(ax1,ax2)= plt.subplots(2,1,sharex=True,figsize=(15,15))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "################################\n",
    "## SECOND DERIVATIVE\n",
    "\n",
    "roots ={}\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "\n",
    "for i in {2}:\n",
    "    ax2.title.set_text(\"Second Derivative of XANES spectrum of \"+str(name)+\"with\"+str(N)+\"of interpolation\")\n",
    "    ax2.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        .3*raw_materials_2nd_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        color='green',label=None,lw=4,alpha=.4\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    \n",
    "    ax2.grid(True)\n",
    "## Now the clamped spline\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        \n",
    "        ## Now find roots in that interval\n",
    "        roots_aux = cs_glue.derivative().roots()\n",
    "        truncate=np.where((roots_aux<=x[-1])*(roots_aux>=x[0]))\n",
    "        roots[str(i)] =roots_aux[truncate]\n",
    "        \n",
    "        d=2\n",
    "        ax2.plot(\n",
    "            x.T,cs_glue(x,d).T,color='red',lw=4,linestyle='--'\n",
    "        )\n",
    "\n",
    "ax2.legend(fontsize=22)\n",
    "orange_patch = mpatches.Patch(color='orange', label='Clamped spline derivative')\n",
    "green_patch = mpatches.Patch(color='green', label=\"Athena 2nd derivative\")\n",
    "\n",
    "ax2.set_xlabel('Energy (eV)',fontsize=22) # X label\n",
    "ax2.set_ylabel('Second derivative',fontsize=22) # Y label\n",
    "ax2.legend(handles=[orange_patch, green_patch],fontsize=22)\n",
    "\n",
    "\n",
    "################################\n",
    "## FIRST DERIVATIVE\n",
    "\n",
    "for i in {2}:\n",
    "    ax1.title.set_text(\"First Derivative of XANES spectrum of \"+str(name)+\"with\"+str(N)+\"of interpolation\")\n",
    "    ax1.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        .3*raw_materials_1st_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        color='green',label=None,lw=4,alpha=.4\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    \n",
    "    ax1.grid(True)\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        ax1.plot(\n",
    "            x.T,cs_glue(x,d).T,color='red',lw=4,linestyle='--'\n",
    "        )\n",
    "        ### PLOT ROOTS\n",
    "        ax1.plot(\n",
    "            roots[str(i)],cs_glue(roots[str(i)],d).T,marker='H',markersize=12,color='b'\n",
    "        )\n",
    "        \n",
    "ax1.legend(fontsize=22)\n",
    "orange_patch = mpatches.Patch(color='orange', label='Clamped spline derivative')\n",
    "green_patch = mpatches.Patch(color='green', label='Athena 1st derivative')\n",
    "ax1.set_xlabel('Energy (eV)',fontsize=22) # X label\n",
    "ax1.set_ylabel('First derivative',fontsize=22) # Y label\n",
    "ax1.legend(handles=[orange_patch, green_patch],fontsize=22)\n",
    "figure_save(\"first_second_Derivative_comparison\"+name+type_interp,figure_extension='png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... the graph of second dervatives look ugly, even though the roots are meaningful. We are going to plot it withouth the second derivative plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "xanes_now = parameters[name]['xanes']\n",
    "E_now =parameters[name]['energy']\n",
    "xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "\n",
    "# ... decomposition\n",
    "decomposition_now = decompositions[name]\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "            \n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "        \n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "\n",
    "xanes_now =np.array(xanes_now[0,start:stop],ndmin=2)\n",
    "domain_now = np.array(E_now[0,start:stop],ndmin=2)\n",
    "\n",
    "f,ax1= plt.subplots(figsize=(15,7))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "################################\n",
    "## SECOND DERIVATIVE\n",
    "\n",
    "roots ={}\n",
    "i =2\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]  \n",
    "        \n",
    "        ## Now find roots in that interval\n",
    "        first_derivative = cs_glue.derivative()\n",
    "        roots_aux = first_derivative.derivative().roots()\n",
    "        truncate=np.where((roots_aux<=x[-1])*(roots_aux>=x[0]))\n",
    "        roots[str(i)] =roots_aux[truncate]\n",
    "        \n",
    "################################\n",
    "## FIRST DERIVATIVE\n",
    "\n",
    "for i in {2}:\n",
    "    ax1.title.set_text(\"First Derivative of XANES spectrum of \"+str(name)+\"with\"+str(N)+\"of interpolation\")\n",
    "    ax1.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        .3*raw_materials_1st_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        color='green',label=None,lw=4,alpha=.4\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    \n",
    "    ax1.grid(True)\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        ax1.plot(\n",
    "            x.T,cs_glue(x,d).T,color='red',lw=4,linestyle='--'\n",
    "        )\n",
    "        ### PLOT ROOTS\n",
    "        ax1.plot(\n",
    "            roots[str(i)],cs_glue(roots[str(i)],d).T,marker='H',markersize=12,color='b',linestyle=''\n",
    "        )\n",
    "        \n",
    "ax1.legend(fontsize=22)\n",
    "orange_patch = mpatches.Patch(color='orange', label='Clamped spline derivative')\n",
    "green_patch = mpatches.Patch(color='green', label='Athena 1st derivative')\n",
    "ax1.set_xlabel('Energy (eV)',fontsize=22) # X label\n",
    "ax1.set_ylabel('First derivative',fontsize=22) # Y label\n",
    "ax1.legend(handles=[orange_patch, green_patch],fontsize=22)\n",
    "figure_save(\"first_Derivative_comparison\"+name+type_interp,figure_extension='png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do different. Let's find the inflection points of the first derivative and compare that with the second derivative in Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "xanes_now = parameters[name]['xanes']\n",
    "E_now =parameters[name]['energy']\n",
    "xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "\n",
    "# ... decomposition\n",
    "decomposition_now = decompositions[name]\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "jumps = decomposition_now['jumps']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "        \n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "jumps_after = jumps[number_splittings_before-1:]\n",
    "    \n",
    "xanes_now =np.array(xanes_now[0,start:stop],ndmin=2)\n",
    "domain_now = np.array(E_now[0,start:stop],ndmin=2)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "\n",
    "where_to_stop_plot = int(.8*peak_loc[str(number_splittings_after-1)]+.2*stop)\n",
    "\n",
    "f,(ax1,ax2)= plt.subplots(2,1,sharex=True, gridspec_kw={'height_ratios': [1, 5]},figsize=(15,8))\n",
    "ax1.set_yticks([0, .5, 1])\n",
    "ax1.plot(E_now[0,:where_to_stop_plot].T,xanes_now[0,:where_to_stop_plot].T,color='blue',lw=4) # Plot xanes spectrum \n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "#To the right of the main peak\n",
    "vector_proportion= []\n",
    "\"We shall plot the xanes measurement and the peak variations\"\n",
    "start_from =0\n",
    "\n",
    "for i in range(start_from,number_splittings_after-1):\n",
    "     ### Now we plot the h_threshold\n",
    "    _,_,h_threshold =\\\n",
    "    oscillation_function(\\\n",
    "                         np.array(E_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                         np.array(xanes_now[0,peak_loc[str(i)]:stop],ndmin=2),\\\n",
    "                         full_computation=False)\n",
    "############################\n",
    "    ### How much of the jump is noise?\n",
    "    proportion_noise_jump=h_threshold/jumps_after[i]\n",
    "    vector_proportion.append(proportion_noise_jump)\n",
    "    #colors = plt.cm.coolwarm(proportion_noise_jump)\n",
    "    colors = ['g','y','r']\n",
    "    # Plot a marker\n",
    "    ax1.plot(E_now[0,peak_loc[str(i)]],xanes_now[0,peak_loc[str(i)]],\\\n",
    "             color=colors[min(int(np.floor(proportion_noise_jump)),2)], marker='o',label=None,  markersize=8,alpha=1,lw=4)\n",
    " \n",
    " ################################\n",
    "## SECOND DERIVATIVE\n",
    "\n",
    "roots ={}\n",
    "i =2\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        \n",
    "        ## Now find roots in that interval\n",
    "        first_derivative = cs_glue.derivative()\n",
    "        roots_aux = first_derivative.derivative().roots()\n",
    "        truncate=np.where((roots_aux<=x[-1])*(roots_aux>=x[0]))\n",
    "        roots[str(i)] =roots_aux[truncate]\n",
    "    \n",
    "################################\n",
    "## FIRST DERIVATIVE\n",
    "\n",
    "for i in {2}:\n",
    "    ax2.title.set_text(\"Inflection points of XANES spectrum of \"+str(name)+\". Interp. points:\"+str(N))\n",
    "    ax2.title.set_fontsize(28)\n",
    "    ax2.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        .3*raw_materials_2nd_derivatives[names_1st_derivatives[0]][0,start:stop].T,\\\n",
    "        color='green',label=None,lw=4,alpha=.4\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    \n",
    "    ax2.grid(True)\n",
    "## Now the clamped spline\n",
    "breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        ax2.plot(\n",
    "            x.T,cs_glue(x,d).T,color='red',lw=4,linestyle='--'\n",
    "        )\n",
    "        ### PLOT ROOTS\n",
    "        ax2.plot(\n",
    "            roots[str(i)],cs_glue(roots[str(i)],d).T,marker='H',markersize=12,color='b',linestyle=''\n",
    "        )\n",
    "        \n",
    "ax2.legend(fontsize=22)\n",
    "orange_patch = mpatches.Patch(color='red', label='Cub. spline (clamped) 1st der.\\n and inflection points')\n",
    "green_patch = mpatches.Patch(color='green', label='Athena 2nd derivative')\n",
    "\n",
    "ax2.set_xlabel('Energy (eV)',fontsize=22) # X label\n",
    "ax2.set_ylabel('First derivative',fontsize=22) # Y label\n",
    "ax2.legend(handles=[orange_patch, green_patch],fontsize=22)\n",
    "figure_save(\"first_Derivative_comparison_withAthena_2nd\"+name+type_interp,figure_extension='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the whole domain, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_interp = 'cubicspline'\n",
    "name = names[0]\n",
    "\n",
    "# UNPACKING...\n",
    "# ... parameters\n",
    "xanes_now = parameters[name]['xanes']\n",
    "E_now =parameters[name]['energy']\n",
    "xanes_now_unpad = parameters[name]['raw_xanes']\n",
    "\n",
    "# ... decomposition\n",
    "decomposition_now = decompositions[name]\n",
    "peak_loc = decomposition_now['peak_location']\n",
    "number_splittings_before = decomposition_now['number_of_splittings_before']\n",
    "number_splittings_after = decomposition_now['number_of_splittings_after']\n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "            \n",
    "start = peak_loc[str(-number_splittings_before)]\n",
    "stop = peak_loc[str(number_splittings_after-1)]\n",
    "        \n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "L = stop\n",
    "finite_difference_first= np.zeros([L])\n",
    "\n",
    "xanes_now =np.array(xanes_now[0,start:stop],ndmin=2)\n",
    "domain_now = np.array(E_now[0,start:stop],ndmin=2)\n",
    "for i in range(2,L-1):\n",
    "    finite_difference_first[i]= (xanes_now[0,i+1]-xanes_now[0,i])/(E_now[0,i+1]-E_now[0,i])\n",
    "\n",
    "for i in {2}:\n",
    "    \n",
    "    ## Now the clamped spline\n",
    "    breakpts,N = normalize_points(parameters,{name},decompositions,k=1+0.1*i,kind='middle_domain')\n",
    "    clamped_outcome=clamped_interpolation(parameters,breakpts,decompositions,{name},printing=False,derivative=[1])\n",
    "\n",
    "    plt.subplot(211)\n",
    "    \n",
    "    ##compute the firs derivative\n",
    "    #y = splev(E_now[0,start:stop], cs, der=1)           \n",
    "    plt.plot(\n",
    "        E_now[0,0:stop],xanes_now[0,0:stop],'--',label =\"Cubic splines $\\Vert\\cdot \\Vert_{\\infty}}$:\",\\\n",
    "        color='red',lw=4\n",
    "    )\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(212)\n",
    "   \n",
    "    ax.plot(\n",
    "        E_derivatives[names_1st_derivatives[0]][0,start:stop].T,.3*raw_materials_1st_derivatives[names_1st_derivatives[0]][0,start:stop].T,color='green',label=None,lw=4\n",
    "    )\n",
    "    y=materials[name][0,start:stop]\n",
    "    plt.title(\"First Derivative of XANES spectrum of \"+str(name)+\"with\"+str(N)+\"of interpolation\",fontsize=28)\n",
    "    plt.grid(True)\n",
    "\n",
    "Number_intervals = len(clamped_outcome[name]['vector_peak_loc'])-1\n",
    "\n",
    "clamped_now = clamped_outcome[name]\n",
    "for i in range(Number_intervals):\n",
    "        # Unpacking....\n",
    "        ### ....clamped_now\n",
    "        cs_glue = clamped_now[str(i)+\"cubic_spline\"]\n",
    "        x = clamped_now[str(i)+\"x_interval\"]\n",
    "        d=1\n",
    "        ax.plot(\n",
    "            x.T,cs_glue(x,d).T,color='orange',lw=4,linestyle='--'\n",
    "        )\n",
    "\n",
    "orange_patch = mpatches.Patch(color='orange', label='Clamped spline derivative')\n",
    "green_patch = mpatches.Patch(color='green', label='Athena derivative')\n",
    "ax.legend(handles=[orange_patch, green_patch],fontsize=22,loc=2)\n",
    "plt.grid(True)\n",
    "figure_save(\"first_Derivative_comparison\"+name+type_interp)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28/parameters[names[0]]['stop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are finaly done :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Raman spectra </center></h1>\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Raman_spectroscopy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR_raman =  os.getcwd()#os.path.dirname(os.path.realpath(__file__))\n",
    "NOTEBOOK_raman = \"Raman_figures\"\n",
    "IMAGES_raman = os.path.join(PROJECT_DIR_raman,\"figures\",NOTEBOOK_raman)\n",
    "\n",
    "### List all the csv files in the directory\n",
    "import glob\n",
    "what_files_raman = glob.glob(PROJECT_DIR+\"/RAMAN/*.csv\")\n",
    "\n",
    "df_raman=pd.DataFrame([])\n",
    "\n",
    "for name_of_file in what_files_raman:\n",
    "    # read  first row with name of components\n",
    "    aux = pd.read_csv(name_of_file,delimiter=',') \n",
    "    #clean up data before \n",
    "    aux.dropna(inplace=True)\n",
    "    \n",
    "    ## Concatenate them all    \n",
    "    df_raman = pd.concat([df_raman,aux],axis=1)\n",
    "\n",
    "data_raman = df_raman.copy()\n",
    "data_raman.dropna(inplace=True)\n",
    "\n",
    "## Plot the data\n",
    "## Getting the name.... I know, I'm doing it in a weird way:\n",
    "names_raman = [str(read[::-1]) for read in what_files_raman]\n",
    "names_raman = [read.partition(\"/\")[0] for read in names_raman]\n",
    "names_raman = [str(read[::-1]) for read in names_raman]\n",
    "names_raman = [read.partition(\".\")[0] for read in names_raman]\n",
    "names_raman = [read.partition(\"_\")[0] for read in names_raman]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A save figures function\n",
    "def figure_save(figure_name, tight_layout=True, figure_extension=\"eps\",resolution=300):\n",
    "    path = os.path.join(IMAGES_raman,figure_name.replace(\" \", \"_\")+\".\"+figure_extension)\n",
    "    print(\"Saving figure as \",figure_name.replace(\" \", \"_\"))\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path,format=figure_extension,dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_raman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are the dictionaries where we will save numpy arrays\n",
    "raw_materials_raman ={}\n",
    "E_raman={}\n",
    "A_raman={}\n",
    "data_val_raman = data_raman.values\n",
    "numb_files_raman = len(what_files_raman)\n",
    "\n",
    "temp_plot= None\n",
    "\n",
    "for i in range(numb_files_raman):\n",
    "    now = data_raman.iloc[:,[2*i,2*i+1]]\n",
    "    now =now.dropna(subset=now.columns)\n",
    "    A_raman[str(i)]= now[now.isnull().any(axis=1)].head()    \n",
    "    E_raman[names_raman[i]] = np.reshape(data_val_raman[:,2*i],(1,-1))\n",
    "    raw_materials_raman[names_raman[i]] = np.reshape(data_val_raman[:,2*i+1],(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I'm gonna change the value of the variable \n",
    "L = len(names)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "#plt.rc('text', usetex=True)\n",
    "for name in names_raman:\n",
    "    plt.plot(\n",
    "        E_raman[name].T,raw_materials_raman[name].T, label = name,lw=4,linestyle='-'\n",
    "    )\n",
    "\n",
    "plt.title('Raman spectrum',size=28)\n",
    "plt.ylabel('$\\mu(E)$',size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.grid(True)\n",
    "figure_save(\"Raman_some_examples\"+names_raman[0])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Normalization and setup of parameters and hyperparameters </h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I'm gonna change the value of the variable \n",
    "\n",
    "materials_raman, shift_height_raman, normalizer_shift_raman =normalization(raw_materials_raman,names_raman,normalizeheight=True)\n",
    "E_padded_raman,materials_raman, stopping_raman = padded_spectra(E_raman,materials_raman,names_raman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_raman ={}\n",
    "\n",
    "for name in names_raman:\n",
    "    parameters_raman[name]={\n",
    "        # The non-normalized xanes\n",
    "        'raw_xanes': raw_materials_raman[name],\n",
    "        'raw_energy': E_raman[name],\n",
    "   \n",
    "        # The normalized xanes and embedded xanes\n",
    "        'xanes': materials_raman[name],\n",
    "        'energy': E_padded_raman[name],\n",
    "\n",
    "        # The normalization and emebdding info\n",
    "        'stop': stopping_raman[name],\n",
    "        'shift_height': shift_height_raman[name],\n",
    "        'normalizer_shift': normalizer_shift_raman[name]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_raman={\n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_h':1,\n",
    "    'lambda_d':1,\n",
    "    \n",
    "    ## These hyperparameters will be used in find_first_peak\n",
    "    'lambda_h_find_1st':4,\n",
    "    'lambda_d_find_1st':1/4,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_d_shrink_1st':1/5, \n",
    "    'initial_oscillation_guess_parameter':10,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'stretching_factor':3,\n",
    "    'iteration_decay':.9,\n",
    "    \n",
    "    ## Type of decay_rate\n",
    "    'decay_rate_type': 'learn_to_trust'\n",
    "}\n",
    "\n",
    "\n",
    "search_conditions_raman = {\n",
    "    'move':'backstep',#'middle_step',\n",
    "    'printing':True,\n",
    "    'polite_guess':{},\n",
    "    'forward':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_split_after_raman, N_split_before_raman=10,10\n",
    "\n",
    "decompositions_raman =\\\n",
    "write_material_peak_properties(\n",
    "    parameters_raman,hyperparameters_raman,search_conditions_raman,names_raman,\\\n",
    "    N_split_after_raman , N_split_before_raman\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marked_peak_spectrum(\n",
    "    parameters_raman,names_raman,decompositions_raman,\\\n",
    "    save_as=names_raman[0]+\"Raman_with_marked_peaks_average-middle_step\",\\\n",
    "    plot_type='raw',loc=1,\n",
    "    title=\"RAMAN spectrum with marked peaks\"\n",
    "    #figure_extension=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>EELS spectra </center></h1>\n",
    "\n",
    "\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Electron_energy_loss_spectroscopy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_eels = \"EELS_figures\"\n",
    "IMAGES = os.path.join(PROJECT_DIR,\"figures\",NOTEBOOK)\n",
    "\n",
    "### List all the csv files in the directory\n",
    "what_files_eels = glob.glob(PROJECT_DIR+\"/EELS/*.csv\")\n",
    "\n",
    "data_eels=pd.DataFrame([])\n",
    "\n",
    "for name_of_file in what_files_eels:\n",
    "    aux = pd.read_csv(name_of_file,delimiter=',') \n",
    "    ## Concatenate them all\n",
    "    aux.dropna(inplace=True)\n",
    "    data_eels = pd.concat([data_eels,aux],axis=1)\n",
    "    \n",
    "data_eels.dropna(inplace=True)\n",
    "\n",
    "## Plot the data\n",
    "## Getting the name.... I know, I'm doing it in a weird way:\n",
    "names_eels = [str(read[::-1]) for read in what_files_eels]\n",
    "names_eels = [read.partition(\"/\")[0] for read in names_eels]\n",
    "names_eels = [str(read[::-1]) for read in names_eels]\n",
    "names_eels = [read.partition(\".\")[0] for read in names_eels]\n",
    "\n",
    "name_in_csv_eels = list(data.columns)\n",
    "### These are the dictionaries where we will save numpy arrays\n",
    "raw_materials_eels ={}\n",
    "E_eels={}\n",
    "A_eels={}\n",
    "data_val_eels = data_eels.values\n",
    "numb_files_eels = len(what_files_eels)\n",
    "\n",
    "for i in range(numb_files_eels):\n",
    "    now = data_eels.iloc[:,[2*i,2*i+1]]\n",
    "    now =now.dropna(subset=now.columns)\n",
    "    A_eels[str(i)]= now[now.isnull().any(axis=1)].head()    \n",
    "    E_eels[names_eels[i]] = np.reshape(data_val_eels[:,2*i],(1,-1))\n",
    "    raw_materials_eels[names_eels[i]] = np.reshape(data_val_eels[:,2*i+1],(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I'm gonna change the value of the variable \n",
    "L = len(names)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "#plt.rc('text', usetex=True)\n",
    "for name in names_eels:\n",
    "    plt.plot(\n",
    "        E_eels[name].T,raw_materials_eels[name].T, label = name,lw=4,linestyle='-'\n",
    "    )\n",
    "\n",
    "plt.title('EELS spectrum',size=28)\n",
    "plt.ylabel('$\\mu(E)$',size=28)\n",
    "plt.xlabel(\"Energy (eV)\", size=28)\n",
    "plt.legend(loc=1,prop={'size':22})\n",
    "plt.grid(True)\n",
    "figure_save(\"EELS_some_examples\"+names_eels[0])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_eels, shift_height_eels, normalizer_shift_eels =normalization(raw_materials_eels,names_eels,normalizeheight=True)\n",
    "E_padded_eels,materials_eels, stopping_eels = padded_spectra(E_eels,materials_eels,names_eels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_eels ={}\n",
    "\n",
    "for name in names_eels:\n",
    "    parameters_eels[name]={\n",
    "        # The non-normalized xanes\n",
    "        'raw_xanes': raw_materials_eels[name],\n",
    "        'raw_energy': E_eels[name],\n",
    "   \n",
    "        # The normalized xanes and embedded xanes\n",
    "        'xanes': materials_eels[name],\n",
    "        'energy': E_padded_eels[name],\n",
    "\n",
    "        # The normalization and emebdding info\n",
    "        'stop': stopping_eels[name],\n",
    "        'shift_height': shift_height_eels[name],\n",
    "        'normalizer_shift': normalizer_shift_eels[name]\n",
    "    }\n",
    "    \n",
    "hyperparameters_eels={\n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_h':1,\n",
    "    'lambda_d':1/2,\n",
    "    \n",
    "    ## These hyperparameters will be used in find_first_peak\n",
    "    'lambda_h_find_1st':4,\n",
    "    'lambda_d_find_1st':1/4,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'lambda_d_shrink_1st':1/2, \n",
    "    'initial_oscillation_guess_parameter':10,\n",
    "    \n",
    "    ## These hyperparameters will be used in \n",
    "    'stretching_factor':3,\n",
    "    'iteration_decay':.9,\n",
    "    \n",
    "    ## Type of decay_rate\n",
    "    'decay_rate_type': 'min_max'\n",
    "}\n",
    "\n",
    "\n",
    "search_conditions_eels = {\n",
    "    'move':'backstep',#'middle_step',\n",
    "    'printing':True,\n",
    "    'polite_guess':{},\n",
    "    'forward':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_eels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_eels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_split_after_eels, N_split_before_eels=10,10\n",
    "\n",
    "decompositions_eels =\\\n",
    "write_material_peak_properties(\n",
    "    parameters_eels,hyperparameters_eels,search_conditions_eels,names_eels,\\\n",
    "     N_split_before_eels,N_split_after_eels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_marked_peak_spectrum(\n",
    "    parameters_eels,names_eels,decompositions_eels,\\\n",
    "    save_as=\"EELS_with_marked_peaks_average-middle_step\",\\\n",
    "    plot_type='raw',\\\n",
    "    title=\"EELS spectrum with marked peaks\",loc=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_metadata": {
   "affiliation": "MathAm-Oil, Sendai, Japan",
   "author": "R. Monteiro",
   "title": "Parallel treatment of training and test set"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
